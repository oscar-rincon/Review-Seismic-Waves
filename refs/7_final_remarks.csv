"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"MAWBADIT","preprint","2019","Kharazmi, E.; Zhang, Z.; Karniadakis, G. E.","Variational Physics-Informed Neural Networks For Solving Partial Differential Equations","","","","10.48550/arXiv.1912.00873","http://arxiv.org/abs/1912.00873","Physics-informed neural networks (PINNs) [31] use automatic differentiation to solve partial differential equations (PDEs) by penalizing the PDE in the loss function at a random set of points in the domain of interest. Here, we develop a Petrov-Galerkin version of PINNs based on the nonlinear approximation of deep neural networks (DNNs) by selecting the {\em trial space} to be the space of neural networks and the {\em test space} to be the space of Legendre polynomials. We formulate the \textit{variational residual} of the PDE using the DNN approximation by incorporating the variational form of the problem into the loss function of the network and construct a \textit{variational physics-informed neural network} (VPINN). By integrating by parts the integrand in the variational form, we lower the order of the differential operators represented by the neural networks, hence effectively reducing the training cost in VPINNs while increasing their accuracy compared to PINNs that essentially employ delta test functions. For shallow networks with one hidden layer, we analytically obtain explicit forms of the \textit{variational residual}. We demonstrate the performance of the new formulation for several examples that show clear advantages of VPINNs over PINNs in terms of both accuracy and speed.","2019-11-27","2024-02-26 16:14:36","2024-02-26 16:14:36","2024-02-26 16:14:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1912.00873 [physics, stat]","","C:\Users\orincon\Zotero\storage\FLU83LXQ\Kharazmi et al. - 2019 - Variational Physics-Informed Neural Networks For S.pdf; C:\Users\orincon\Zotero\storage\LPYQ8F4E\1912.html","","","Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Mathematics - Numerical Analysis; Physics - Computational Physics; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1912.00873","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X3NYRL6R","journalArticle","2020","Chen, Feiyu; Sondak, David; Protopapas, Pavlos; Mattheakis, Marios; Liu, Shuheng; Agarwal, Devansh; Giovanni, Marco Di","NeuroDiffEq: A Python package for solving differential equations with neural networks","Journal of Open Source Software","","2475-9066","10.21105/joss.01931","https://joss.theoj.org/papers/10.21105/joss.01931","Chen et al., (2020). NeuroDiffEq: A Python package for solving differential equations with neural networks. Journal of Open Source Software, 5(46), 1931, https://doi.org/10.21105/joss.01931","2020-02-19","2024-07-23 00:35:06","2024-07-23 00:35:06","2024-07-23 00:34:59","1931","","46","5","","","NeuroDiffEq","","","","","","","en","","","","","joss.theoj.org","","","","C:\Users\orincon\Zotero\storage\CHWKM2R6\Chen et al. - 2020 - NeuroDiffEq A Python package for solving differen.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RSBPMBVT","journalArticle","2021","Lu, Lu; Meng, Xuhui; Mao, Zhiping; Karniadakis, George Em","DeepXDE: A Deep Learning Library for Solving Differential Equations","SIAM Review","","0036-1445","10.1137/19M1274067","https://epubs.siam.org/doi/10.1137/19M1274067","Physics-informed neural networks (PINNs) have become a popular choice for solving high-dimensional partial differential equations (PDEs) due to their excellent approximation power and generalization ability. Recently, extended PINNs (XPINNs) based on domain decomposition methods have attracted considerable attention due to their effectiveness in modeling multiscale and multiphysics problems and their parallelization. However, theoretical understanding of their convergence and generalization properties remains unexplored. In this study, we take an initial step towards understanding how and when XPINNs outperform PINNs. Specifically, for general multilayer PINNs and XPINNs, we first provide a prior generalization bound via the complexity of the target functions in the PDE problem and a posterior generalization bound via the posterior matrix norms of the networks after optimization. Moreover, based on our bounds, we analyze the conditions under which XPINNs improve generalization. Concretely, our theory shows that the key building block of XPINN, namely, the domain decomposition, introduces a tradeoff for generalization. On the one hand, XPINNs decompose the complex PDE solution into several simple parts, which decreases the complexity needed to learn each part and boosts generalization. On the other hand, decomposition leads to less training data being available in each subdomain, and hence  such a model is typically prone to overfitting and may become less generalizable. Empirically, we choose five PDEs to show when XPINNs perform better than, similar to, or worse than PINNs, hence demonstrating and justifying our new theory.","2021-01","2024-07-23 00:36:33","2024-07-23 00:36:33","2024-07-23 00:36:19","208-228","","1","63","","SIAM Rev.","DeepXDE","","","","","","","","","","","","epubs.siam.org (Atypon)","","Publisher: Society for Industrial and Applied Mathematics","","C:\Users\orincon\Zotero\storage\LLG7WCQ3\Lu et al. - 2021 - DeepXDE A Deep Learning Library for Solving Diffe.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MRUQ2B82","webpage","2021","Zubov, Kirill; McCarthy, Zoe; Ma, Yingbo; Calisto, Francesco; Pagliarino, Valerio; Azeglio, Simone; Bottero, Luca; Luján, Emmanuel; Sulzer, Valentin; Bharambe, Ashutosh; Vinchhi, Nand; Balakrishnan, Kaushik; Upadhyay, Devesh; Rackauckas, Chris","NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with Error Approximations","arXiv.org","","","","https://arxiv.org/abs/2107.09443v1","Physics-informed neural networks (PINNs) are an increasingly powerful way to solve partial differential equations, generate digital twins, and create neural surrogates of physical models. In this manuscript we detail the inner workings of NeuralPDE.jl and show how a formulation structured around numerical quadrature gives rise to new loss functions which allow for adaptivity towards bounded error tolerances. We describe the various ways one can use the tool, detailing mathematical techniques like using extended loss functions for parameter estimation and operator discovery, to help potential users adopt these PINN-based techniques into their workflow. We showcase how NeuralPDE uses a purely symbolic formulation so that all of the underlying training code is generated from an abstract formulation, and show how to make use of GPUs and solve systems of PDEs. Afterwards we give a detailed performance analysis which showcases the trade-off between training techniques on a large set of PDEs. We end by focusing on a complex multiphysics example, the Doyle-Fuller-Newman (DFN) Model, and showcase how this PDE can be formulated and solved with NeuralPDE. Together this manuscript is meant to be a detailed and approachable technical report to help potential users of the technique quickly get a sense of the real-world performance trade-offs and use cases of the PINN techniques.","2021-07-19","2024-07-23 00:37:09","2024-07-23 00:37:09","2024-07-23 00:37:09","","","","","","","NeuralPDE","","","","","","","en","","","","","","","","","C:\Users\orincon\Zotero\storage\K4864CJX\Zubov et al. - 2021 - NeuralPDE Automating Physics-Informed Neural Netw.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXC4YNUR","conferencePaper","2023","Bafghi, Reza Akbarian; Raissi, Maziar","PINNs-Torch: Enhancing Speed and Usability of Physics-Informed Neural Networks with PyTorch","","","","","https://openreview.net/forum?id=nl1ZzdHpab","Physics-informed neural networks (PINNs) stand out for their ability in supervised learning tasks that align with physical laws, especially nonlinear partial differential equations (PDEs). In this paper, we introduce ""PINNs-Torch"", a Python package that accelerates PINNs implementation using the PyTorch framework and streamlines user interaction by abstracting PDE issues. While we utilize PyTorch's dynamic computational graph for its flexibility, we mitigate its computational overhead in PINNs by compiling it to static computational graphs. In our assessment across 8 diverse examples, covering continuous, discrete, forward, and inverse configurations, naive PyTorch is slower than TensorFlow; however, when integrated with CUDA Graph and JIT compilers, training speeds can increase by up to 9 times relative to TensorFlow implementations. Additionally, through a real-world example, we highlight situations where our package might not deliver speed improvements. For community collaboration and future developments, our package code is accessible at: https://github.com/rezaakb/pinns-torch.","2023-10-31","2024-07-23 00:37:57","2024-07-23 00:37:57","2024-07-16 16:24:57","","","","","","","PINNs-Torch","","","","","","","en","","","","","openreview.net","","","","C:\Users\orincon\Zotero\storage\LSAWYYZQ\Bafghi y Raissi - 2023 - PINNs-Torch Enhancing Speed and Usability of Phys.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","The Symbiosis of Deep Learning and Differential Equations III","","","","","","","","","","","","","","",""
"AAVVYIA2","journalArticle","1995","Pao, Yoh-Han; Phillips, Stephen M.","The functional link net and learning optimal control","Neurocomputing","","0925-2312","10.1016/0925-2312(95)00066-F","https://www.sciencedirect.com/science/article/pii/092523129500066F","We present a strategy for learning optimal control. The approach uses functional-link neural network implementations which have several beneficial properties giving advantages over the more common generalized delta rule implementations. The learning task is decomposed into three parts: identification and monitoring, one-step-ahead control generation, and control path optimization. Each of these parts is accomplished with its own functional-link net and these are coordinated to provide the real-time learning of the optimal control path.","1995-10-01","2024-07-23 00:38:11","2024-07-23 00:38:11","2024-06-14 20:18:13","149-164","","2","9","","Neurocomputing","","Control and Robotics, Part II","","","","","","","","","","","ScienceDirect","","","","C:\Users\orincon\Zotero\storage\LCV336HU\092523129500066F.html","","","Functional-link net; Neural net control; Optimal control; Real-time learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""