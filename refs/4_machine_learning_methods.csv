"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"V6XUH65Y","preprint","2021","Li, Zongyi; Kovachki, Nikola; Azizzadenesheli, Kamyar; Liu, Burigede; Bhattacharya, Kaushik; Stuart, Andrew; Anandkumar, Anima","Fourier Neural Operator for Parametric Partial Differential Equations","","","","10.48550/arXiv.2010.08895","http://arxiv.org/abs/2010.08895","The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.","2021-05-16","2024-02-06 16:04:52","2024-03-14 03:35:40","2024-02-06 16:04:52","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2010.08895 [cs, math]","","C:\Users\orincon\Zotero\storage\UP6MS2FA\Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf; C:\Users\orincon\Zotero\storage\U8MH2GYE\2010.html","","","Computer Science - Machine Learning; Mathematics - Numerical Analysis","","","","","","","","","","","","","","","","","","","arXiv:2010.08895","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"93I8ZQPV","preprint","2020","Li, Zongyi; Kovachki, Nikola; Azizzadenesheli, Kamyar; Liu, Burigede; Bhattacharya, Kaushik; Stuart, Andrew; Anandkumar, Anima","Neural Operator: Graph Kernel Network for Partial Differential Equations","","","","10.48550/arXiv.2003.03485","http://arxiv.org/abs/2003.03485","The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.","2020-03-06","2024-02-22 22:22:17","2024-02-22 22:22:17","2024-02-22 22:22:17","","","","","","","Neural Operator","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2003.03485 [cs, math, stat]","","C:\Users\orincon\Zotero\storage\62G8G5RY\Li et al. - 2020 - Neural Operator Graph Kernel Network for Partial .pdf; C:\Users\orincon\Zotero\storage\KFV6JKE2\2003.html","","","Computer Science - Machine Learning; Mathematics - Numerical Analysis; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2003.03485","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"77RQSL95","journalArticle","2021","Blechschmidt, Jan; Ernst, Oliver G.","Three ways to solve partial differential equations with neural networks — A review","GAMM-Mitteilungen","","1522-2608","10.1002/gamm.202100006","https://onlinelibrary.wiley.com/doi/abs/10.1002/gamm.202100006","Neural networks are increasingly used to construct numerical solution methods for partial differential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman–Kac formula and methods based on the solution of backward stochastic differential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.","2021","2024-02-22 21:47:19","2024-02-22 21:47:19","2024-02-22 21:47:19","e202100006","","2","44","","","","","","","","","","en","© 2021 The Authors. GAMM - Mitteilungen published by Wiley-VCH GmbH.","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/gamm.202100006","","C:\Users\orincon\Zotero\storage\9BL4ZG4Y\Blechschmidt and Ernst - 2021 - Three ways to solve partial differential equations.pdf; C:\Users\orincon\Zotero\storage\UJ8H3XUH\gamm.html","","","backward differential equation; curse of dimensionality; Feynman–Kac; Hamilton–Jacobi–Bellman equations; neural networks; partial differential equation; PINN; stochastic process","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3QVP9AST","book","2016","Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron","Deep Learning","","978-0-262-33737-3","","","","An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.","2016-11-10","2024-02-22 21:12:53","2024-02-22 21:12:53","","","801","","","","","","","","","","MIT Press","","en","","","","","Google Books","","Google-Books-ID: omivDQAAQBAJ","","","https://books.google.com.co/books?id=omivDQAAQBAJ","","Computers / Artificial Intelligence / General; Computers / Computer Science; Computers / Data Science / Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XQLT4CM6","journalArticle","2015","LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey","Deep learning","Nature","","1476-4687","10.1038/nature14539","https://doi.org/10.1038/nature14539","Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.","2015-05-01","2024-02-22 16:24:24","2024-02-22 16:24:24","","436-444","","7553","521","","Nature","","","","","","","","","","","","","","","","","C:\Users\orincon\Zotero\storage\T3P6QVXZ\LeCun et al. - 2015 - Deep learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PH9FJE9F","journalArticle","2019","Raissi, M.; Perdikaris, P.; Karniadakis, G. E.","Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations","Journal of Computational Physics","","0021-9991","10.1016/j.jcp.2018.10.045","https://www.sciencedirect.com/science/article/pii/S0021999118307125","We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.","2019-02-01","2023-09-22 01:14:56","2024-02-08 18:48:11","2023-09-22 01:14:56","686-707","","","378","","Journal of Computational Physics","Physics-informed neural networks","","","","","","","","","","","","ScienceDirect","","","","C:\Users\orincon\Zotero\storage\2QMTE3FH\Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf; C:\Users\orincon\Zotero\storage\8RCRZNKL\S0021999118307125.html","","Physics-informed neural network (PINN)","Data-driven scientific computing; Machine learning; Nonlinear dynamics; Predictive modeling; Runge–Kutta methods","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PFKMGMH3","journalArticle","1998","Lagaris, I.E.; Likas, A.; Fotiadis, D.I.","Artificial neural networks for solving ordinary and partial differential equations","IEEE Transactions on Neural Networks","","1941-0093","10.1109/72.712178","https://ieeexplore.ieee.org/document/712178","We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations (ODE), to systems of coupled ODE and also to partial differential equations (PDE). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galerkin finite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.","1998-09","2024-02-06 16:27:55","2024-02-06 16:27:55","2024-02-06 16:27:55","987-1000","","5","9","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Neural Networks","","C:\Users\orincon\Zotero\storage\XSIA9LNA\712178.html; C:\Users\orincon\Zotero\storage\BCVSIKWB\Lagaris et al. - 1998 - Artificial neural networks for solving ordinary an.pdf","","","Artificial neural networks; Boundary conditions; Boundary value problems; Differential equations; Digital signal processors; Feedforward neural networks; Finite element methods; Moment methods; Neural networks; Partial differential equations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FMC59VW4","journalArticle","2020","Karimpouli, Sadegh; Tahmasebi, Pejman","Physics informed machine learning: Seismic wave equation","Geoscience Frontiers","","1674-9871","10.1016/j.gsf.2020.07.007","https://www.sciencedirect.com/science/article/pii/S1674987120301717","Similar to many fields of sciences, recent deep learning advances have been applied extensively in geosciences for both small- and large-scale problems. However, the necessity of using large training data and the ‘black box’ nature of learning have limited them in practice and difficult to interpret. Furthermore, including the governing equations and physical facts in such methods is also another challenge, which entails either ignoring the physics or simplifying them using unrealistic data. To address such issues, physics informed machine learning methods have been developed which can integrate the governing physics law into the learning process. In this work, a 1-dimensional (1D) time-dependent seismic wave equation is considered and solved using two methods, namely Gaussian process (GP) and physics informed neural networks. We show that these meshless methods are trained by smaller amount of data and can predict the solution of the equation with even high accuracy. They are also capable of inverting any parameter involved in the governing equation such as wave velocity in our case. Results show that the GP can predict the solution of the seismic wave equation with a lower level of error, while our developed neural network is more accurate for velocity (P- and S-wave) and density inversion.","2020-11-01","2024-02-01 23:38:43","2024-02-01 23:38:43","2024-02-01 23:38:43","1993-2001","","6","11","","Geoscience Frontiers","Physics informed machine learning","","","","","","","","","","","","ScienceDirect","","","","C:\Users\orincon\Zotero\storage\TLCZRAWN\Karimpouli and Tahmasebi - 2020 - Physics informed machine learning Seismic wave eq.pdf; C:\Users\orincon\Zotero\storage\L8R35MEI\S1674987120301717.html","","","Gaussian process (GP); Optimization; Physics informed machine learning (PIML); Seismic wave","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XRV55KM","journalArticle","2021","Karniadakis, George Em; Kevrekidis, Ioannis G.; Lu, Lu; Perdikaris, Paris; Wang, Sifan; Yang, Liu","Physics-informed machine learning","Nature Reviews Physics","","25225820","10.1038/s42254-021-00314-5","","Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.","2021-06","2023-09-22 01:09:19","2024-07-22 21:54:30","","422-440","","6","3","","","","","","","","","","","","","","","","","Publisher: Springer Nature","","C:\Users\orincon\Zotero\storage\AD5R44VB\Karniadakis et al. - 2021 - Physics-informed machine learning.pdf; C:\Users\orincon\Zotero\storage\KYP7DZKX\Karniadakis et al. - 2021 - Physics-informed machine learning.pdf","","","Applied mathematics; Computational science","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3M2QIYMU","journalArticle","2018","Raissi, Maziar; Karniadakis, George Em","Hidden physics models: Machine learning of nonlinear partial differential equations","Journal of Computational Physics","","0021-9991","10.1016/j.jcp.2017.11.039","https://www.sciencedirect.com/science/article/pii/S0021999117309014","While there is currently a lot of enthusiasm about “big data”, useful data is usually “small” and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from small data. In particular, we introduce hidden physics models, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier–Stokes, Schrödinger, Kuramoto–Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.","2018-03-15","2024-07-12 20:34:37","2024-07-12 20:34:37","2024-07-12 20:34:37","125-141","","","357","","Journal of Computational Physics","Hidden physics models","","","","","","","","","","","","ScienceDirect","","","","C:\Users\orincon\Zotero\storage\FFJU96N2\S0021999117309014.html; C:\Users\orincon\Zotero\storage\B86VWNDH\Raissi y Karniadakis - 2018 - Hidden physics models Machine learning of nonline.pdf","","","Bayesian modeling; Fractional equations; Probabilistic machine learning; Small data; System identification; Uncertainty quantification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VJ7IQMLU","journalArticle","2018","Raissi, Maziar; Perdikaris, Paris; Karniadakis, George Em","Numerical Gaussian Processes for Time-Dependent and Nonlinear Partial Differential Equations","SIAM Journal on Scientific Computing","","1064-8275","10.1137/17M1120762","https://epubs.siam.org/doi/abs/10.1137/17M1120762","Implicit-explicit (IMEX) schemes have been widely used, especially in conjunction with spectral methods, for the time integration of spatially discretized partial differential equations (PDEs) of diffusion-convection type. Typically, an implicit scheme is used for the diffusion term and an explicit scheme is used for the convection term. Reaction-diffusion problems can also be  approximated in this manner. In this work we systematically analyze the performance of such schemes, propose improved new schemes, and pay particular attention to their relative performance in the context of fast multigrid algorithms and of aliasing reduction for spectral methods.For the prototype linear advection-diffusion equation, a stability analysis for first-, second-, third-, and fourth-order multistep IMEX schemes is performed. Stable schemes permitting large time steps for a wide variety of problems and yielding appropriate decay of high frequency error modes are identified. Numerical experiments demonstrate that weak decay of high frequency modes can lead to extra iterations on the finest grid when using multigrid computations with finite difference spatial discretization, and to aliasing when using spectral collocation for spatial discretization. When this behavior occurs, use of weakly damping schemes such as the popular combination of Crank–Nicolson with second-order Adams–Bashforth is discouraged and better alternatives are proposed.Our findings are demonstrated on several examples.","2018-01","2024-07-12 20:35:44","2024-07-12 20:35:44","2024-07-12 20:35:44","A172-A198","","1","40","","SIAM J. Sci. Comput.","","","","","","","","","","","","","epubs.siam.org (Atypon)","","Publisher: Society for Industrial and Applied Mathematics","","C:\Users\orincon\Zotero\storage\7VVI5YHJ\Raissi et al. - 2018 - Numerical Gaussian Processes for Time-Dependent an.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""