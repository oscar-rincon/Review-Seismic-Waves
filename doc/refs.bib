
@article{seron-1990,
author = {Serón, F. J. and Sanz, F. J. and Kindelán, M. and Badal, J. I.},
title = {Finite-element method for elastic wave propagation},
journal = {Communications in Applied Numerical Methods},
volume = {6},
number = {5},
pages = {359-368},
doi = {https://doi.org/10.1002/cnm.1630060505},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.1630060505},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cnm.1630060505},
abstract = {Abstract The object of this work is to analyse the computational aspects of the finite-element method for the elastic wave equations. The necessary numerical techniques are analysed from the point of view of accuracy, performance and storage requirements when implemented in scalar and vector processors with large storage capacity. The method is implemented on an IBM 3090 with vector facility. For this implementation we consider five different time integration schemes (explicit and implicit central difference, Houbolt, constant average acceleration and Wilson), and in the implicit case, both direct (Gaussian decomposition) and iterative (successive over-relaxation, Jacobi semi-iterative, Jacobi conjugate gradient) sparse linear systems solvers. These solvers are taken from the ITPACK2C and ESSL libraries using in each case the adequate representation scheme; skyline, row-wise and compressed diagonal. From our results it is concluded that constant average acceleration and explicit central difference are the most adequate integration methods and Jacobi conjugate gradient is the most efficient solver.},
year = {1990}
}

@Article{app13031312,
AUTHOR = {Liu, Siqin and Zhou, Zhusheng and Zeng, Weizu},
TITLE = {Simulation of Elastic Wave Propagation Based on Meshless Generalized Finite Difference Method with Uniform Random Nodes and Damping Boundary Condition},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {1312},
URL = {https://www.mdpi.com/2076-3417/13/3/1312},
ISSN = {2076-3417},
ABSTRACT = {When the grid-based finite difference (FD) method is used for elastic wavefield forward modeling, it is inevitable that the grid divisions will be inconsistent with the actual velocity interface, resulting in problems related to the stepped grid diffraction and inaccurate travel time of reflected waves. The generalized finite difference method (GFDM), which is based on the Taylor series expansion and weighted least square fitting, solves these problems. The partial derivative of the unknown parameters in the differential equation is represented by the linear combination of the function values of adjacent nodes. In this study, the Poisson disk node generation algorithm and the centroid Voronoi node adjustment algorithm were combined to obtain an even and random node distribution. The generated nodes fit the internal boundary more accurately for model discretization, without the presence of diffracted waves caused by the stepped grid. To avoid the instability caused by the introduction of boundary conditions, a Cerjan damping boundary condition was proposed for boundary reflection processing. The test results generated by the different models showed that the generalized finite difference method can effectively solve the problems related to inaccurate travel time of reflection waves and stepped grid diffraction.},
DOI = {10.3390/app13031312}
}


@book{Aki,
   abstract = {Second edition. "First paperback impression; corrected printing"--Title page verso},
   author = {Keiiti Aki and Paul G. Richards},
   isbn = {9781891389634},
   pages = {700},
   year ={2002},
   title = {Quantitative seismology},
}


@book{Achenbach1973,
   abstract = {The propagation of mechanical disturbances in solids is of interest in many branches of the physical scienses and engineering. This book aims to present an account of the theory of wave propagation in elastic solids. The material is arranged to present an exposition of the basic concepts of mechanical wave propagation within a one-dimensional setting and a discussion of formal aspects of elastodynamic theory in three dimensions, followed by chapters expounding on typical wave propagation phenomena, such as radiation, reflection, refraction, propagation in waveguides, and diffraction. The treatment necessarily involves considerable mathematical analysis. The pertinent mathematical techniques are, however, discussed at some length. Introduction -- One-dimensional motion of an elastic continuum -- The linearized theory of elasticity -- Elastodynamic theory -- Elastic waves in an unbound medium -- Plane harmonic waves in elastic half-spaces -- Harmonic waves in waveguides -- Forced motions of a half-space -- Transient waves in layers and rods -- Diffraction of waves by a slit -- Thermal and viscoelastic effects, and effects of anisotrophy and non-linearity.},
   author = {J. D. Achenbach},
   isbn = {0720423503},
   pages = {425},
   publisher = {North-Holland Pub. Co},
   title = {Wave propagation in elastic solids,},
   year = {1973},
}

@book{pujol,
   author = {Jose Pujol},
   publisher = {cambridge university press},
   title = {Elastic Wave Propagation
and Generation in
Seismology},
   year = {2003},
}

@article{Liu2023,
   abstract = {When the grid-based finite difference (FD) method is used for elastic wavefield forward modeling, it is inevitable that the grid divisions will be inconsistent with the actual velocity interface, resulting in problems related to the stepped grid diffraction and inaccurate travel time of reflected waves. The generalized finite difference method (GFDM), which is based on the Taylor series expansion and weighted least square fitting, solves these problems. The partial derivative of the unknown parameters in the differential equation is represented by the linear combination of the function values of adjacent nodes. In this study, the Poisson disk node generation algorithm and the centroid Voronoi node adjustment algorithm were combined to obtain an even and random node distribution. The generated nodes fit the internal boundary more accurately for model discretization, without the presence of diffracted waves caused by the stepped grid. To avoid the instability caused by the introduction of boundary conditions, a Cerjan damping boundary condition was proposed for boundary reflection processing. The test results generated by the different models showed that the generalized finite difference method can effectively solve the problems related to inaccurate travel time of reflection waves and stepped grid diffraction.},
   author = {Siqin Liu and Zhusheng Zhou and Weizu Zeng},
   doi = {10.3390/app13031312},
   issn = {20763417},
   issue = {3},
   journal = {Applied Sciences (Switzerland)},
   keywords = {Cerjan damping boundary condition,centroid Voronoi,elastic wave modeling,generalized finite difference method (GFDM)},
   month = {2},
   publisher = {MDPI},
   title = {Simulation of Elastic Wave Propagation Based on Meshless Generalized Finite Difference Method with Uniform Random Nodes and Damping Boundary Condition},
   volume = {13},
   year = {2023},
}


@article{Ursula,
   author = {Ursula Iturrarán-Viveros and Francisco J. Sánchez-Sesma},
   doi = {10.1007/978-3-030-10475-7_6-1},
   issn = {1871756X},
   journal = {Encyclopedia of Earth Sciences Series},
   publisher = {Springer Science and Business Media B.V.},
   title = {Seismic Wave Propagation in Real Media: Numerical Modeling Approaches},
   year = {2020},
}

@book{Carcione2002,
   author = {José Carcione},
   title = {Wave Propagation in Anisotropic, Anelastic, Porous and Electromagnetic Media},
   year = {2002},
}

@incollection{Moczo,
title = {The Finite-Difference Time-Domain Method for Modeling of Seismic Wave Propagation},
editor = {Ru-Shan Wu and Valerie Maupin and Renata Dmowska},
series = {Advances in Geophysics},
publisher = {Elsevier},
volume = {48},
pages = {421-516},
year = {2007},
booktitle = {Advances in Wave Propagation in Heterogenous Earth},
issn = {0065-2687},
doi = {https://doi.org/10.1016/S0065-2687(06)48008-0},
url = {https://www.sciencedirect.com/science/article/pii/S0065268706480080},
author = {Peter Moczo and Johan O.A. Robertsson and Leo Eisner},
keywords = {Anisotropy, Attenuation, Earthquake motion, Earthquake source dynamics, Finite-difference method, Free surface, Non-reflecting boundaries, Numerical modeling, Optimally accurate operators, Seismic waves},
abstract = {We present a review of the recent development in finite-difference time-domain modeling of seismic wave propagation and earthquake motion. The finite-difference method is a robust numerical method applicable to structurally complex media. Due to its relative accuracy and computational efficiency it is the dominant method in modeling earthquake motion and it also is becoming increasingly more important in the seismic industry and for structural modeling. We first introduce basic formulations and properties of the finite-difference schemes including promising recent advances. Then we address important topics as material discontinuities, realistic attenuation, anisotropy, the planar free surface boundary condition, free-surface topography, wavefield excitation (including earthquake source dynamics), non-reflecting boundaries, and memory optimization and parallelization.}
}

@book{Chapman2004,
   abstract = {1. Introduction -- 2. Basic wave propagation -- 3. Transforms -- 4. Review of continuum mechanics and elastic waves -- 5. Asymptotic ray theory -- 6. Rays at an interface -- 7. Differential systems for stratified media -- 8. Inverse transforms for stratified media -- 9. Canonical signals -- 10. Generalizations of ray theory.},
   author = {Chris H. Chapman},
   isbn = {052181538X},
   pages = {608},
   publisher = {Cambridge University Press},
   title = {Fundamentals of seismic wave propagation},
   year = {2004},
}

@misc{Seriani2020,
   abstract = {The numerical modeling of mechanical waves is currently a fundamental tool for the study and investigation of their propagation in media with heterogeneous physical properties and/or complex geometry, as, in these cases, analytical methods are usually not applicable. These techniques are used in geophysics (geophysical interpretation, subsoil imaging, development of new methods of exploration), seismology (study of earthquakes, regional and global seismology, accurate calculation of synthetic seismograms), in the development of new methods for ultrasonic diagnostics in materials science (non-destructive methods) and medicine (acoustic tomography). In this paper we present a review of numerical methods that have been developed and are currently used. In particular we review the key concepts and pioneering ideas behind finite-difference methods, pseudospectral methods, finite-volume methods, Galerkin continuous and discontinuous finite-element methods (classical or based on spectral interpolation), and still others such as physics-compatible, and multiscale methods. We focus on their formulations in time domain along with the main temporal discretization schemes. We present the theory and implementation for some of these methods. Moreover, their computational characteristics are evaluated in order to aid the choice of the method for each practical situation.},
   author = {G. Seriani and S. P. Oliveira},
   doi = {10.1007/s40766-020-00009-0},
   issn = {18269850},
   issue = {9},
   journal = {Rivista del Nuovo Cimento},
   month = {9},
   pages = {459-514},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Numerical modeling of mechanical wave propagation},
   volume = {43},
   year = {2020},
}

@article{guarin-2023,
   abstract = {There is an increasing interest in the study of metamaterials and periodic materials across disciplines. These are anisotropic and their properties present directionality. For example, the wave speed depends on the propagation direction. Furthermore, they are heterogeneous, and their directionality depends on their spectra. Common approaches to describe anisotropy have been used in the large-wavelength approximation corresponding to static properties. Here we present an anisotropy measure based on the dynamic behavior. It receives dispersion surfaces from Bloch analyses and outputs a curve/surface with bulk directionality encoded on it. We present results for elastodynamics, but it is applicable to other phenomena.},
   author = {Nicolás Guarín-Zapata and Camilo Valencia and Juan Gomez},
   doi = {10.1080/15376494.2023.2226958},
   issn = {15376532},
   journal = {Mechanics of Advanced Materials and Structures},
   keywords = {Bloch analysis,Periodic material,anisotropy,dispersion relation,phononic crystal},
   publisher = {Taylor and Francis Ltd.},
   title = {Analysis of the directionality on periodic materials},
   year = {2023},
}

@misc{FEniCS,
   author = {Hans Petter Langtangen and Anders Logg},
   title = {Solving PDEs in Python The FEniCS Tutorial I},
   url = {http://www.springer.com/series/13548},
 year = {2016},
}

@software{solidspy,
 title = {SolidsPy: 2D-Finite Element Analysis with Python},
 version = {1.1.0},
 author = {Guarín-Zapata, Nicolás and Gómez, Juan},
 year = 2023,
 keywords = {Python, Finite elements, Scientific computing, Computational mechanics},
 abstract = {SolidsPy is a simple finite element analysis code for 2D elasticity
 problems. The code uses as input simple-to-create text files defining a model
 in terms of nodal, element, material and load data.},
 url = {https://github.com/AppliedMechanics-EAFIT/SolidsPy},
 doi = {https://doi.org/10.5281/zenodo.7694030}
}


@article{Padovani,
author = {Padovani, E. and Priolo, E. and Seriane, G.},
title = {Low and High Order Finite Element Method: Experience in Seismic Modeling},
journal = {Journal of Computational Acoustics},
volume = {02},
number = {04},
pages = {371-422},
year = {1994},
doi = {10.1142/S0218396X94000233},
URL ={https://doi.org/10.1142/S0218396X94000233},
}

@book{Tarantola,
author = {Tarantola, Albert},
title = {Inverse Problem Theory and Methods for Model Parameter Estimation},
publisher = {Society for Industrial and Applied Mathematics},
year = {2005},
doi = {10.1137/1.9780898717921},
address = {},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898717921},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898717921}
}

@book{Igel2017,
   author = {Heiner Igel},
   publisher = {Oxford University Press},
   title = {Computational seismology: a practical introduction},
   year = {2017},
}


@book{Schuster,
   author = {Gerard Schuster},
   publisher = {Society of Exploration Geophysicists,},
   title = {Seismic Inversion},
   year = {2017},
}

@book{Moczo2014,
   abstract = {Among all the numerical methods in seismology, the finite-difference (FD) technique provides the best balance of accuracy and computational efficiency. This book offers a comprehensive introduction to FD and its applications to earthquake motion. Using a systematic tutorial approach, the book requires only undergraduate degree-level mathematics and provides a user-friendly explanation of the relevant theory. It explains FD schemes for solving wave equations and elastodynamic equations of motion in heterogeneous media, and provides an introduction to the rheology of viscoelastic and elastoplastic media. It also presents an advanced FD time-domain method for efficient numerical simulations of earthquake ground motion in realistic complex models of local surface sedimentary structures. Accompanied by a suite of online resources to help put the theory into practice, this is a vital resource for professionals and academic researchers using numerical seismological techniques, and graduate students in earthquake seismology, computational and numerical modelling, and applied mathematics. A systematic tutorial introduction to the finite-difference (FD) numerical modelling technique for professionals, academic researchers, and graduate students in seismology. Cover; Half title; Epigraph; Title; Copyright; Dedication; Contents; Acknowledgements; Selected symbols; Abbreviations; Mathematical notation; Greek symbols; Latin symbols; 1 Introduction; Part I Mathematical-physical model; 2 Basic mathematical-physical model; 2.1 Medium; 2.2 Governing equation: equation of motion; 2.2.1 Strong form; 2.2.2 Weak form; 2.2.3 Integral strong form; 2.2.4 Concluding remark; 2.3 Constitutive law: stress-strain relation; 2.3.1 Elastic continuum; 2.3.2 Viscoelastic continuum; 2.4 Strong-form formulations of equations; 2.4.1 Displacement-stress formulation 2.4.2 Displacement formulation2.4.3 Displacement-velocity-stress formulation; 2.4.4 Velocity-stress formulation; 2.5 Boundary conditions; 2.5.1 Free surface; 2.5.2 Welded material interface; 2.6 Initial conditions; 2.7 Wavefield source (wavefield excitation); 3 Rheological models of a continuum; 3.1 Basic rheological models; 3.1.1 Hooke elastic solid; 3.1.2 Newton viscous liquid; 3.1.3 Saint-Venant plastic solid; 3.2 Combined rheological models; 3.3 Viscoelastic continuum and its rheological models; 3.3.1 Stress-strain and strain-stress relations in a viscoelastic continuum 3.3.1.1 Preliminary considerations3.3.1.2 General theory in 1D; 3.3.2 Maxwell and Kelvin-Voigt bodies; 3.3.3 Zener body (standard linear solid); 3.3.4 Phase velocity in elastic and viscoelastic continua; 3.3.5 Measure of dissipation and attenuation in a viscoelastic continuum; 3.3.6 Attenuation in a Zener body; 3.3.7 Generalized Zener body (GZB); 3.3.8 Generalized Maxwell body (GMB-EK); 3.3.9 Equivalence of GZB and GMB-EK; 3.3.10 Anelastic functions (memory variables); 3.3.11 Anelastic coefficients and unrelaxed modulus; 3.3.12 Attenuation and phase velocity in GMB-EK/GZB continuum 3.3.13 Stress-strain relation in 3D3.4 Elastoplastic continuum; 3.4.1 Simplest elastoplastic bodies; 3.4.2 Iwan elastoplastic model for hysteretic stress-strain behaviour; 3.4.2.1 Preliminary considerations; 3.4.2.2 Iwan model; 3.4.2.3 Iwan model and Masing rules; 3.4.2.4 Determination of parameters for Iwan model; 3.4.2.5 Note on damping; 3.4.2.6 Concluding remark; 4 Earthquake source; 4.1 Dynamic model of an earthquake source; 4.1.1 Boundary conditions for dynamic shear faulting; 4.1.2 Friction law; 4.1.2.1 Linear slip-weakening friction law; 4.1.2.2 Rate- and state-dependent friction law 4.2 Kinematic model of an earthquake source4.2.1 Point source; 4.2.2 Finite-fault kinematic source; Part II The finite-difference method; 5 Time-domain numerical methods; 5.1 Introduction; 5.2 Fourier pseudo-spectral method; 5.3 Spectral element method; 5.4 Spectral discontinuous Galerkin scheme with ADER time integration; 5.5 Hybrid methods; 6 Brief introduction to the finite-difference method; 6.1 Space-time grids; 6.1.1 Cartesian grid; 6.1.2 Uniform, nonuniform and discontinuous grids; 6.1.3 Structured and unstructured grids; 6.1.4 Space-time locations of field variables},
   author = {Peter Moczo and Jozef Kristek and Martin Gális},
   isbn = {9781107028814},
   pages = {365},
   title = {The Finite-Difference Modelling of Earthquake Motions : Waves and Ruptures},
   year = {2014},
}




@article{Zhou2021,
   author = {Hongyu Zhou and Yang Liu and Jing Wang},
   doi = {10.29382/eqs-2021},
   journal = {Earthquake Science},
   keywords = {Courant-Friedrichs-Lewy numbers,acoustic wave equation,finite-difference,stability condition,vari-able length},
   pages = {123-236},
   title = {Acoustic finite-difference modeling beyond conventi-onal Courant-Friedrichs-Lewy stability limit: Approach based on variable-length temporal and spatial operators},
   volume = {34},
   year = {2021},
}
@misc{Virieux1986,
   abstract = {I present a finite-difference method for modeling P-SV wave propagation in heterogeneous media. This is an extension of the method I previously proposed for modeling SH-wave propagation by using velocity and stress in a discrete grid. The two components of the velocity cannot be defined at the same node for a complete staggered grid: the stability condition and the P-wave phase velocity dispersion curve do not depend on the Poisson's ratio, while the S-wave phase velocity dispersion curve behavior is rather insensitive to the Poisson's ratio. Therefore, the same code used for elastic media can be used for liquid media, where S-wave ve-INTRODUCTION},
   author = {Jean Virieux},
   title = {p-sv wave propagation in heterogeneous media: Velocity-stress finite-difference method},
   url = {http://library.seg.org/},
   year = {1986},
}

@article{Raissi2019,
   abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
   author = {M. Raissi and P. Perdikaris and G. E. Karniadakis},
   doi = {10.1016/J.JCP.2018.10.045},
   issn = {0021-9991},
   journal = {Journal of Computational Physics},
   keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge–Kutta methods},
   month = {2},
   pages = {686-707},
   publisher = {Academic Press},
   title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
   volume = {378},
   year = {2019},
}
@article{Song2022,
   abstract = {Wavefield reconstruction inversion (WRI) formulates a PDE-constrained optimization problem to reduce cycle skipping in full-waveform inversion (FWI). WRI is often implemented by solving for the frequency-domain representation of the wavefield using the finite-difference method. The approach requires matrix inversions and affords limited flexibility to accommodate irregular model geometries. On the other hand, the physics-informed neural network (PINN) uses the underlying physical laws as loss functions to train the neural network (NN) to provide flexible continuous functional approximations of the solutions without matrix inversions. By including a data-constrained term in the loss function, the trained NN can reconstruct a wavefield that simultaneously fits the recorded data and satisfies the Helmholtz equation for a given initial velocity model. Using the predicted wavefields, we rely on a small-size NN to predict the velocity using the reconstructed wavefield. In this velocity prediction NN, spatial coordinates are used as input data to the network, and the scattered Helmholtz equation is used to define the loss function. After we train this network, we are able to predict the velocity in the domain of interest. We develop this PINN-based WRI method and demonstrate its potential using a part of the Sigsbee2A model and a modified Marmousi model. The results show that the PINN-based WRI is able to invert for a reasonable velocity with very limited iterations and frequencies, which can be used in a subsequent FWI application.},
   author = {Chao Song and Tariq A. Alkhalifah},
   doi = {10.1109/TGRS.2021.3123122},
   issn = {15580644},
   journal = {IEEE Transactions on Geoscience and Remote Sensing},
   keywords = {Frequency-domain,Helmholtz equation,full-waveform inversion (FWI),physics-informed neural network (PINN),wavefield reconstruction inversion (WRI)},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Wavefield Reconstruction Inversion via Physics-Informed Neural Networks},
   volume = {60},
   year = {2022},
}


@article{liu_simulation_2023,
	title = {Simulation of {Elastic} {Wave} {Propagation} {Based} on {Meshless} {Generalized} {Finite} {Difference} {Method} with {Uniform} {Random} {Nodes} and {Damping} {Boundary} {Condition}},
	volume = {13},
	doi = {10.3390/app13031312},
	abstract = {When the grid-based finite difference (FD) method is used for elastic wavefield forward modeling, it is inevitable that the grid divisions will be inconsistent with the actual velocity interface, resulting in problems related to the stepped grid diffraction and inaccurate travel time of reflected waves. The generalized finite difference method (GFDM), which is based on the Taylor series expansion and weighted least square fitting, solves these problems. The partial derivative of the unknown parameters in the differential equation is represented by the linear combination of the function values of adjacent nodes. In this study, the Poisson disk node generation algorithm and the centroid Voronoi node adjustment algorithm were combined to obtain an even and random node distribution. The generated nodes fit the internal boundary more accurately for model discretization, without the presence of diffracted waves caused by the stepped grid. To avoid the instability caused by the introduction of boundary conditions, a Cerjan damping boundary condition was proposed for boundary reflection processing. The test results generated by the different models showed that the generalized finite difference method can effectively solve the problems related to inaccurate travel time of reflection waves and stepped grid diffraction.},
	number = {3},
	journal = {Applied Sciences (Switzerland)},
	author = {Liu, Siqin and Zhou, Zhusheng and Zeng, Weizu},
	month = feb,
	year = {2023},
	note = {Publisher: MDPI},
	keywords = {centroid Voronoi, Cerjan damping boundary condition, elastic wave modeling, generalized finite difference method (GFDM)},
	file = {Liu et al. - 2023 - Simulation of Elastic Wave Propagation Based on Me.pdf:C\:\\Users\\osrin\\Zotero\\storage\\BCTSY2MD\\Liu et al. - 2023 - Simulation of Elastic Wave Propagation Based on Me.pdf:application/pdf},
}


@article{rasht-behesht_physics-informed_2022,
	title = {Physics-{Informed} {Neural} {Networks} ({PINNs}) for {Wave} {Propagation} and {Full} {Waveform} {Inversions}},
	volume = {127},
	copyright = {© 2022. American Geophysical Union. All Rights Reserved.},
	issn = {2169-9356},
	url ={https://onlinelibrary.wiley.com/doi/abs/10.1029/2021JB023120},
	doi = {10.1029/2021JB023120},
	abstract = {We propose a new approach to the solution of the wave propagation and full waveform inversions (FWIs) based on a recent advance in deep learning called physics-informed neural networks (PINNs). In this study, we present an algorithm for PINNs applied to the acoustic wave equation and test the method with both forward models and FWI case studies. These synthetic case studies are designed to explore the ability of PINNs to handle varying degrees of structural complexity using both teleseismic plane waves and seismic point sources. PINNs' meshless formalism allows for a flexible implementation of the wave equation and different types of boundary conditions. For instance, our models demonstrate that PINN automatically satisfies absorbing boundary conditions, a serious computational challenge for common wave propagation solvers. Furthermore, a priori knowledge of the subsurface structure can be seamlessly encoded in PINNs' formulation. We find that the current state-of-the-art PINNs provide good results for the forward model, even though spectral element or finite difference methods are more efficient and accurate. More importantly, our results demonstrate that PINNs yield excellent results for inversions on all cases considered and with limited computational complexity. We discuss the current limitations of the method with complex velocity models as well as strategies to overcome these challenges. Using PINNs as a geophysical inversion solver offers exciting perspectives, not only for the full waveform seismic inversions, but also when dealing with other geophysical datasets (e.g., MT, gravity) as well as joint inversions because of its robust framework and simple implementation.},
	language = {en},
	number = {5},
	urldate = {2024-01-30},
	journal = {Journal of Geophysical Research: Solid Earth},
	author = {Rasht-Behesht, Majid and Huber, Christian and Shukla, Khemraj and Karniadakis, George Em},
	year = {2022},
	keywords = {acoustic wave propagation, deep learning, full waveform inversion, physics-informed neural networks},
	pages = {e2021JB023120},
	annote = {e2021JB023120 2021JB023120},
	file = {Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/KXLU7S7Y/2021JB023120.html:text/html;Submitted Version:/home/orincon/snap/zotero-snap/common/Zotero/storage/FSESZLFX/Rasht-Behesht et al. - 2022 - Physics-Informed Neural Networks (PINNs) for Wave .pdf:application/pdf},
}



@article{moseley_deep_2020,
	title = {Deep learning for fast simulation of seismic waves in complex media},
	volume = {11},
	issn = {1869-9510},
	url = {https://se.copernicus.org/articles/11/1527/2020/},
	doi = {10.5194/se-11-1527-2020},
	abstract = {The simulation of seismic waves is a core task in many geophysical applications. Numerical methods such as finite difference (FD) modelling and spectral element methods (SEMs) are the most popular techniques for simulating seismic waves, but disadvantages such as their computational cost prohibit their use for many tasks. In this work, we investigate the potential of deep learning for aiding seismic simulation in the solid Earth sciences. We present two deep neural networks which are able to simulate the seismic response at multiple locations in horizontally layered and faulted 2-D acoustic media an order of magnitude faster than traditional finite difference modelling. The first network is able to simulate the seismic response in horizontally layered media and uses a WaveNet network architecture design. The second network is significantly more general than the first and is able to simulate the seismic response in faulted media with arbitrary layers, fault properties and an arbitrary location of the seismic source on the surface of the media, using a conditional autoencoder design. We test the sensitivity of the accuracy of both networks to different network hyperparameters and show that the WaveNet network can be retrained to carry out fast seismic inversion in the same media. We find that are there are challenges when extending our methods to more complex, elastic and 3-D Earth models; for example, the accuracy of both networks is reduced when they are tested on models outside of their training distribution. We discuss further research directions which could address these challenges and potentially yield useful tools for practical simulation tasks.},
	language = {English},
	number = {4},
	urldate = {2024-01-30},
	journal = {Solid Earth},
	author = {Moseley, Ben and Nissen-Meyer, Tarje and Markham, Andrew},
	month = aug,
	year = {2020},
	note = {Publisher: Copernicus GmbH},
	pages = {1527--1549},
	file = {Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/FDLLGK6R/Moseley et al. - 2020 - Deep learning for fast simulation of seismic waves.pdf:application/pdf},
}


@phdthesis{moseley_physics-informed_2022,
	title = {Physics-informed machine learning: from concepts to real-world applications},
	shorttitle = {Physics-informed machine learning},
	url = {https://ora.ox.ac.uk/objects/uuid:b790477c-771f-4926-99c6-d2f9d248cb23},
	abstract = {{\textless}p{\textgreater}Machine learning (ML) has caused a fundamental shift in how we practice science, with many now placing learning from data at the focal point of their research. As the complexity of the scientific problems we want to study increases, and the amount of data generated by today's scientific experiments grows, ML is helping to automate, accelerate and enhance traditional workflows.{\textless}/p{\textgreater} {\textless}p{\textgreater}Emerging at the forefront of this revolution is a field called scientific machine learning (SciML). The central goal of SciML is to more tightly combine existing scientific understanding with ML, generating powerful ML algorithms which are informed by our prior knowledge.{\textless}/p{\textgreater} {\textless}p{\textgreater}A plethora of approaches exist for incorporating scientific principles into ML and expectations are rising for SciML to address some of the biggest challenges in science. However, the field is burgeoning and many questions are still arising. A major one is whether SciML approaches can scale to more complex, real-world problems. Much SciML research is at a proof-of-concept stage, where techniques are validated on simplified, toy problems. Yet, understanding how well they scale to more complex problems is essential for them to become widely applicable.{\textless}/p{\textgreater} {\textless}p{\textgreater}This question is of central focus in this thesis. Firstly, multiple different physics-informed ML approaches are designed for three complex, real-world, domain-specific case studies taken from the fields of lunar science and geophysics, and their performance and scalability is assessed. Secondly, the scalability of physics-informed neural networks, a popular and general SciML approach, for solving differential equations with large domains and high frequency solutions is evaluated and improved. Common observations across these studies are discussed, and significant advantages and underlying limitations are identified, highlighting the importance of designing scalable SciML techniques.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-01-30},
	school = {University of Oxford},
	author = {Moseley, B.},
	year = {2022},
	file = {Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/QKQMLP4U/Moseley - 2022 - Physics-informed machine learning from concepts t.pdf:application/pdf},
}



@article{Robertsson,
	address = {Cham},
	title = {Numerical {Methods}, {Finite} {Difference}},
	isbn = {978-3-030-10475-7},
	url = {http://link.springer.com/10.1007/978-3-030-10475-7_135-1},
	language = {en},
	urldate = {2024-02-01},
	booktitle = {Encyclopedia of {Solid} {Earth} {Geophysics}},
	publisher = {Springer International Publishing},
	author = {Robertsson, Johan O. A. and Blanch, Joakim O.},
	editor = {Gupta, Harsh K.},
	year = {2020},
	doi = {10.1007/978-3-030-10475-7_135-1},
	note = {Series Title: Encyclopedia of Earth Sciences Series},
	pages = {1--9},
	file = {Robertsson and Blanch - 2020 - Numerical Methods, Finite Difference.pdf:/home/orincon/snap/zotero-snap/common/Zotero/storage/Q4CWX7TB/Robertsson and Blanch - 2020 - Numerical Methods, Finite Difference.pdf:application/pdf},
}


@article{karimpouli_physics_2020,
	title = {Physics informed machine learning: {Seismic} wave equation},
	volume = {11},
	issn = {1674-9871},
	shorttitle = {Physics informed machine learning},
	url = {https://www.sciencedirect.com/science/article/pii/S1674987120301717},
	doi = {10.1016/j.gsf.2020.07.007},
	abstract = {Similar to many fields of sciences, recent deep learning advances have been applied extensively in geosciences for both small- and large-scale problems. However, the necessity of using large training data and the ‘black box’ nature of learning have limited them in practice and difficult to interpret. Furthermore, including the governing equations and physical facts in such methods is also another challenge, which entails either ignoring the physics or simplifying them using unrealistic data. To address such issues, physics informed machine learning methods have been developed which can integrate the governing physics law into the learning process. In this work, a 1-dimensional (1D) time-dependent seismic wave equation is considered and solved using two methods, namely Gaussian process (GP) and physics informed neural networks. We show that these meshless methods are trained by smaller amount of data and can predict the solution of the equation with even high accuracy. They are also capable of inverting any parameter involved in the governing equation such as wave velocity in our case. Results show that the GP can predict the solution of the seismic wave equation with a lower level of error, while our developed neural network is more accurate for velocity (P- and S-wave) and density inversion.},
	number = {6},
	urldate = {2024-02-01},
	journal = {Geoscience Frontiers},
	author = {Karimpouli, Sadegh and Tahmasebi, Pejman},
	month = nov,
	year = {2020},
	keywords = {Gaussian process (GP), Optimization, Physics informed machine learning (PIML), Seismic wave},
	pages = {1993--2001},
	file = {Karimpouli and Tahmasebi - 2020 - Physics informed machine learning Seismic wave eq.pdf:/home/orincon/snap/zotero-snap/common/Zotero/storage/TLCZRAWN/Karimpouli and Tahmasebi - 2020 - Physics informed machine learning Seismic wave eq.pdf:application/pdf;ScienceDirect Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/L8R35MEI/S1674987120301717.html:text/html},
}


@article{karniadakis_physics-informed_2021,
	title = {Physics-informed machine learning},
	volume = {3},
	copyright = {2021 Springer Nature Limited},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-021-00314-5},
	doi = {10.1038/s42254-021-00314-5},
	abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
	language = {en},
	number = {6},
	urldate = {2024-02-05},
	journal = {Nature Reviews Physics},
	author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
	month = jun,
	year = {2021},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computational science},
	pages = {422--440},
	file = {Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/86K6KKQS/Karniadakis et al. - 2021 - Physics-informed machine learning.pdf:application/pdf},
}



@article{rash_2022,
	title = {Physics-{Informed} {Neural} {Networks} ({PINNs}) for {Wave} {Propagation} and {Full} {Waveform} {Inversions}},
	volume = {127},
	copyright = {© 2022. American Geophysical Union. All Rights Reserved.},
	issn = {2169-9356},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021JB023120},
	doi = {10.1029/2021JB023120},
	abstract = {We propose a new approach to the solution of the wave propagation and full waveform inversions (FWIs) based on a recent advance in deep learning called physics-informed neural networks (PINNs). In this study, we present an algorithm for PINNs applied to the acoustic wave equation and test the method with both forward models and FWI case studies. These synthetic case studies are designed to explore the ability of PINNs to handle varying degrees of structural complexity using both teleseismic plane waves and seismic point sources. PINNs' meshless formalism allows for a flexible implementation of the wave equation and different types of boundary conditions. For instance, our models demonstrate that PINN automatically satisfies absorbing boundary conditions, a serious computational challenge for common wave propagation solvers. Furthermore, a priori knowledge of the subsurface structure can be seamlessly encoded in PINNs' formulation. We find that the current state-of-the-art PINNs provide good results for the forward model, even though spectral element or finite difference methods are more efficient and accurate. More importantly, our results demonstrate that PINNs yield excellent results for inversions on all cases considered and with limited computational complexity. We discuss the current limitations of the method with complex velocity models as well as strategies to overcome these challenges. Using PINNs as a geophysical inversion solver offers exciting perspectives, not only for the full waveform seismic inversions, but also when dealing with other geophysical datasets (e.g., MT, gravity) as well as joint inversions because of its robust framework and simple implementation.},
	language = {en},
	number = {5},
	urldate = {2024-01-30},
	journal = {Journal of Geophysical Research: Solid Earth},
	author = {Rasht-Behesht, Majid and Huber, Christian and Shukla, Khemraj and Karniadakis, George Em},
	year = {2022},
	keywords = {deep learning, acoustic wave propagation, full waveform inversion, physics-informed neural networks},
	pages = {e2021JB023120},
	annote = {e2021JB023120 2021JB023120},
	file = {Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/KXLU7S7Y/2021JB023120.html:text/html;Submitted Version:/home/orincon/snap/zotero-snap/common/Zotero/storage/FSESZLFX/Rasht-Behesht et al. - 2022 - Physics-Informed Neural Networks (PINNs) for Wave .pdf:application/pdf},
}


@article{ren_seismicnet_2024,
	title = {{SeismicNet}: {Physics}-informed neural networks for seismic wave modeling in semi-infinite domain},
	volume = {295},
	issn = {0010-4655},
	shorttitle = {{SeismicNet}},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465523003557},
	doi = {10.1016/j.cpc.2023.109010},
	abstract = {Recently, there has been an increasing interest in leveraging physics-informed neural networks (PINNs) for modeling dynamical systems. However, limited studies have been conducted along this horizon on seismic wave modeling tasks. A critical challenge is that these geophysical problems are typically defined in large domains (i.e., semi-infinite), which leads to high computational costs. We present a new PINN model for seismic wave modeling in semi-infinite domain without the need for labeled data. Specifically, the absorbing boundary condition is introduced into the network as a soft regularizer for handling truncated boundaries. To scale up, we consider a sequential training strategy via temporal domain decomposition to improve the scalability of the network and solution accuracy. Moreover, we design a novel surrogate modeling strategy to account for parametric loading, which estimates the wave propagation in semi-infinite domain given the seismic loading at different locations. Various numerical experiments are implemented to evaluate the performance of the proposed PINN model in the context of forward modeling of seismic wave propagation. In particular, we use diverse material distributions to test the versatility of this approach. The results demonstrate excellent solution accuracy under distinctive scenarios.},
	urldate = {2024-02-05},
	journal = {Computer Physics Communications},
	author = {Ren, Pu and Rao, Chengping and Chen, Su and Wang, Jian-Xun and Sun, Hao and Liu, Yang},
	month = feb,
	year = {2024},
	keywords = {Absorbing boundary conditions, Domain decomposition, Forward simulation, Physics-informed neural networks, Seismic wave modeling, Semi-infinite domain},
	pages = {109010},
	file = {ScienceDirect Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/NF8687CF/S0010465523003557.html:text/html;Versión enviada:/home/orincon/snap/zotero-snap/common/Zotero/storage/XU8HGXP9/Ren et al. - 2024 - SeismicNet Physics-informed neural networks for s.pdf:application/pdf},
}



@misc{li_fourier_2021,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/UP6MS2FA/Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf;arXiv.org Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/U8MH2GYE/2010.html:text/html},
}


@article{lagaris_artificial_1998,
	title = {Artificial neural networks for solving ordinary and partial differential equations},
	volume = {9},
	issn = {1941-0093},
	url = {https://ieeexplore.ieee.org/document/712178},
	doi = {10.1109/72.712178},
	abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations (ODE), to systems of coupled ODE and also to partial differential equations (PDE). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galerkin finite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.},
	number = {5},
	urldate = {2024-02-06},
	journal = {IEEE Transactions on Neural Networks},
	author = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
	month = sep,
	year = {1998},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Artificial neural networks, Boundary conditions, Boundary value problems, Differential equations, Digital signal processors, Feedforward neural networks, Finite element methods, Moment methods, Neural networks, Partial differential equations},
	pages = {987--1000},
	file = {IEEE Xplore Abstract Record:/home/orincon/snap/zotero-snap/common/Zotero/storage/XSIA9LNA/712178.html:text/html;IEEE Xplore Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/BCVSIKWB/Lagaris et al. - 1998 - Artificial neural networks for solving ordinary an.pdf:application/pdf},
}

@inproceedings{abadi_tensorflow_2016,
	title = {\{{TensorFlow}\}: {A} {System} for \{{Large}-{Scale}\} {Machine} {Learning}},
	isbn = {978-1-931971-33-1},
	shorttitle = {\{{TensorFlow}\}},
	url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
	language = {en},
	urldate = {2024-02-19},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
	file = {Full Text PDF:C\:\\Users\\osrin\\Zotero\\storage\\9RJH29DJ\\Abadi et al. - 2016 - TensorFlow A System for Large-Scale Machine L.pdf:application/pdf},
}

@article{virieux_review_2011,
author = {Virieux, Jean and Calandra, Henri and Plessix, René-Edouard},
title = {A review of the spectral, pseudo-spectral, finite-difference and finite-element modelling techniques for geophysical imaging},
journal = {Geophysical Prospecting},
volume = {59},
number = {5},
pages = {794-813},
keywords = {Electromagnetism, Imaging, Modelling, Seismic},
doi = {https://doi.org/10.1111/j.1365-2478.2011.00967.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2478.2011.00967.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2478.2011.00967.x},
abstract = {ABSTRACT Modelling methods are nowadays at the heart of any geophysical interpretation approach. These are heavily relied upon by imaging techniques in elastodynamics and electromagnetism, where they are crucial for the extraction of subsurface characteristics from ever larger and denser datasets. While high-frequency or one-way approximations are very powerful and efficient, they reach their limits when complex geological settings and solutions of full equations are required at finite frequencies. A review of three important formulations is carried out here: the spectral method, which is very efficient and accurate but generally restricted to simple earth structures and often layered earth structures; the pseudo-spectral, finite-difference and finite-volume methods based on strong formulation of the partial differential equations, which are easy to implement and currently represent a good compromise between accuracy, efficiency and flexibility and the continuous or discontinuous Galerkin finite-element methods that are based on the weak formulation, which lead to more accurate earth representations and therefore to more accurate solutions, although with higher computational costs and more complex use. The choice between these different approaches is still difficult and depends on the applications. Guidelines are given here through discussion of the requirements for imaging/inversion.},
year = {2011}
}



@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-02-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	file = {Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/FAT3ZNYK/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@article{cuomo_scientific_2022,
	title = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}: {Where} we are and {What}’s {Next}},
	volume = {92},
	issn = {1573-7691},
	shorttitle = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}},
	url = {https://doi.org/10.1007/s10915-022-01939-z},
	doi = {10.1007/s10915-022-01939-z},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
	language = {en},
	number = {3},
	urldate = {2024-02-19},
	journal = {Journal of Scientific Computing},
	author = {Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	month = jul,
	year = {2022},
	keywords = {Numerical methods, Uncertainty, Deep Neural Networks, Nonlinear equations, Partial Differential Equations, Physics–Informed Neural Networks, Scientific Machine Learning},
	pages = {88},
	file = {Exported Items.bib:/home/orincon/snap/zotero-snap/common/Zotero/storage/FF7PZ4ZC/Exported Items.bib:text/x-bibtex;Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/K6R2UNBH/Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:application/pdf},
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
	file = {Paszke et al. - Automatic differentiation in PyTorch.pdf:/home/orincon/snap/zotero-snap/common/Zotero/storage/C4UMT8RF/Paszke et al. - Automatic differentiation in PyTorch.pdf:application/pdf},
}

@article{alterman_propagation_1968,
	title = {Propagation of elastic waves in layered media by finite difference methods},
	volume = {58},
	issn = {0037-1106},
	url = {https://doi.org/10.1785/BSSA0580010367},
	doi = {10.1785/BSSA0580010367},
	abstract = {A finite difference equation formulation for the equations of elasticity is presented and applied to the problem of a layered half-space with a buried point source emitting a compressional pulse. Complete theoretical seismograms for the horizontal and vertical components of displacement are obtained. The results for a specific case are compared with those found by a completely different method in order to check the validity of the finite difference methods. The agreement is excellent. The effect of different mesh sizes on the theoretical seismograms is studied next and a suitable grid system selected for the applications that follow. The development of Rayleigh waves on the surface of a half-space and the change of the Rayleigh wave with depth and pulse width are examined. The problem of a layered half-space with a high velocity bottom is considered and the refraction arrivals on the surface and on the interface are studied. The problem of interface waves on the surface separating two semiinfinite media is also examined. Interface waves are found when the physical parameters lie both inside and outside the region determined by the Stoneley equation. Finally, a series of theoretical seismograms for a layered half-space showing the variation of the surface waves as a function of depth and of the density in the lower medium is presented.},
	number = {1},
	urldate = {2024-02-20},
	journal = {Bulletin of the Seismological Society of America},
	author = {Alterman, Z. and Karal, Jr., F. C.},
	month = feb,
	year = {1968},
	pages = {367--398},
	file = {Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/B4TFM55L/Propagation-of-elastic-waves-in-layered-media-by.html:text/html},
}

@article{madariaga_dynamics_1976,
	title = {Dynamics of an expanding circular fault},
	volume = {66},
	issn = {0037-1106},
	url = {https://doi.org/10.1785/BSSA0660030639},
	doi = {10.1785/BSSA0660030639},
	abstract = {We study a plane circular model of a frictional fault using numerical methods. The model is dynamic since we specify the effective stress at the fault. In one model we assume that the fault appears instantaneously in the medium; in another, that the rupture nucleates at the center and that rupture proceeds at constant subsonic velocity until it suddenly stops. The total source slip is larger at the center and the rise time is also longer at the center of the fault. The dynamic slip overshoots the static slip by 15 to 35 per cent. As a consequence, the stress drop is larger than the effective stress and the apparent stress is less than one half the effective stress.The far-field radiation is discussed in detail. We distinguish three spectral regions. First, the usual constant low-frequency level. Second, an intermediate region controlled by the fault size and, finally, the high-frequency asymptote. The central region includes the corner frequency and is quite complicated. The corner frequency is shown to be inversely proportional to the width of the far-field displacement pulse which, in turn, is related to the time lag between the stopping phases. The average corner frequency of S waves v0s is related to the final source radius, a, by v0s = 0.21 β/α. The corner frequency of P waves is larger than v0s by an average factor of 1.5.},
	number = {3},
	urldate = {2024-02-20},
	journal = {Bulletin of the Seismological Society of America},
	author = {Madariaga, Raul},
	month = jun,
	year = {1976},
	pages = {639--666},
	file = {Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/U4QVB82I/Dynamics-of-an-expanding-circular-fault.html:text/html},
}

@article{frankel_three-dimensional_1992,
	title = {A three-dimensional simulation of seismic waves in the {Santa} {Clara} {Valley}, {California}, from a {Loma} {Prieta} aftershock},
	volume = {82},
	issn = {0037-1106},
	url = {https://doi.org/10.1785/BSSA0820052045},
	doi = {10.1785/BSSA0820052045},
	abstract = {The finite-difference method is used to propagate elastic waves through a 3-D model of the Santa Clara Valley, an alluvium-filled basin that underlies the city of San Jose, California. The model was based on depth to bedrock information from water wells in the area. The simulation corresponded to a region 30 (east-west) by 22 (north-south) by 6 (depth) km and contained about 4 million grid points. Synthetic seismograms from the simulation are accurate at frequencies up to 1 Hz. Motions from a magnitude 4.4 aftershock of the Loma Prieta earthquake were modeled. Snapshots of ground motion and synthetic seismograms from the simulation are presented. The simulation illustrates S-to-surface-wave conversion at the edges of the basin and the large amplitude and long duration of ground motion in the basin compared to the surrounding rock. Love waves produced at the edge of the basin are the largest arrivals in the transverse synthetics. Because of the slow group velocity of the Love waves, sites near the center of the basin have longer durations of significant motions than basin sites near the valley edges. Sites near the center of the basin also show larger peak amplitudes on the transverse component. Array analysis of the synthetic seismograms indicates that Love waves tend to propagate parallel to the eastern and western edges of the valley. Rayleigh waves are produced along the southern margin of the basin from incident S waves. Large radial motions occur where a Rayleigh wave impinges on the northeast margin of the valley. Some Rayleigh waves travel westward across the basin, after being scattered from the eastern edge of the valley. Synthetic seismograms from the simulation have similar peak amplitudes as seismograms recorded by the Sunnyvale dense array for this aftershock, although the duration of the tranverse component is not matched by the synthetic seismogram, using this basin model. The simulation indicates that the Love waves observed on the actual seismograms were produced by conversion of incident S waves at the southern margin of the Santa Clara Valley. 2-D simulations show how the S-to-Love-wave conversion is affected by the angle of incidence of the S wave and the sharpness of the velocity transition between the alluvium and bedrock. As more accurate basin models are developed, 3-D simulations should become valuable for predicting ground motions in sedimentary basins for future large earthquakes.},
	number = {5},
	urldate = {2024-02-20},
	journal = {Bulletin of the Seismological Society of America},
	author = {Frankel, Arthur and Vidale, John},
	month = oct,
	year = {1992},
	pages = {2045--2074},
	file = {Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/DI8FV8DW/A-three-dimensional-simulation-of-seismic-waves-in.html:text/html},
}

@inproceedings{lehmann_fourier_2023,
	title = {Fourier {Neural} {Operator} {Surrogate} {Model} to {Predict} {3D} {Seismic} {Waves} {Propagation}},
	url = {http://arxiv.org/abs/2304.10242},
	doi = {10.7712/120223.10339.20362},
	abstract = {With the recent rise of neural operators, scientific machine learning offers new solutions to quantify uncertainties associated with high-fidelity numerical simulations. Traditional neural networks, such as Convolutional Neural Networks (CNN) or Physics-Informed Neural Networks (PINN), are restricted to the prediction of solutions in a predefined configuration. With neural operators, one can learn the general solution of Partial Differential Equations, such as the elastic wave equation, with varying parameters. There have been very few applications of neural operators in seismology. All of them were limited to two-dimensional settings, although the importance of three-dimensional (3D) effects is well known. In this work, we apply the Fourier Neural Operator (FNO) to predict ground motion time series from a 3D geological description. We used a high-fidelity simulation code, SEM3D, to build an extensive database of ground motions generated by 30,000 different geologies. With this database, we show that the FNO can produce accurate ground motion even when the underlying geology exhibits large heterogeneities. Intensity measures at moderate and large periods are especially well reproduced. We present the first seismological application of Fourier Neural Operators in 3D. Thanks to the generalizability of our database, we believe that our model can be used to assess the influence of geological features such as sedimentary basins on ground motion, which is paramount to evaluating site effects.},
	urldate = {2024-02-12},
	author = {Lehmann, Fanny and Gatti, Filippo and Bertin, Michaël and Clouteau, Didier},
	year = {2023},
	note = {arXiv:2304.10242 [physics]},
	keywords = {Physics - Geophysics, Computer Science - Machine Learning},
	pages = {297--310},
	file = {arXiv.org Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/C2LKZN4Y/2304.html:text/html;Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/JV8LF5IV/Lehmann et al. - 2023 - Fourier Neural Operator Surrogate Model to Predict.pdf:application/pdf},
}

@article{baydin_automatic_2017,
	title = {Automatic differentiation in machine learning: a survey},
	volume = {18},
	issn = {1532-4435},
	shorttitle = {Automatic differentiation in machine learning},
	abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Baydin, Atılım Günes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	month = jan,
	year = {2017},
	keywords = {backpropagation, differentiable programming},
	pages = {5595--5637},
	file = {Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/TV96P8ND/Baydin et al. - 2017 - Automatic differentiation in machine learning a s.pdf:application/pdf},
}

 @article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {978-0-262-33737-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	language = {en},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
	note = {Google-Books-ID: omivDQAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Computer Science, Computers / Data Science / Machine Learning},
}


@article{lino_current_2023,
	title = {Current and emerging deep-learning methods for the simulation of fluid dynamics},
	volume = {479},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspa.2023.0058},
	doi = {10.1098/rspa.2023.0058},
	abstract = {Over the last decade, deep learning (DL), a branch of machine learning, has experienced rapid progress. Powerful tools for tasks that have been traditionally complex to automate have been developed, such as image synthesis and natural language processing. In the context of simulating fluid dynamics, this has led to a series of novel DL methods for replacing or augmenting conventional numerical solvers. We broadly classify these methods into physics- and data-driven methods. Physics-driven methods, generally, tune a DL model to provide an analytical and differentiable solution to a given fluid dynamics problem by minimizing the residuals of the governing partial differential equations. Data-driven methods provide a fast and approximate solution to any fluid dynamics problem that shares some physical properties with the observations used when tuning the DL model’s parameters. Meanwhile, the symbiosis of numerical solvers and DL has led to promising results in turbulence modelling and accelerating iterative solvers. However, these methods present some challenges. Exclusively data-driven flow simulators often suffer from poor extrapolation, error accumulation in time-dependent simulations, as well as difficulties in training against turbulent flows. Substantial effort is, therefore, being invested into approaches that may improve the current state of the art.},
	number = {2275},
	urldate = {2024-02-19},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Lino, Mario and Fotiadis, Stathi and Bharath, Anil A. and Cantwell, Chris D.},
	month = jul,
	year = {2023},
	note = {Publisher: Royal Society},
	keywords = {computational fluid dynamics, data-driven fluid dynamics, deep learning, deep neural networks, physics-informed neural networks, turbulence modelling},
	pages = {20230058},
	file = {Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/YECXYB3L/Lino et al. - 2023 - Current and emerging deep-learning methods for the.pdf:application/pdf},
}

@article{deng_physics-informed_2023,
	title = {Physics-informed machine learning in prognostics and health management: {State} of the art and challenges},
	volume = {124},
	issn = {0307-904X},
	shorttitle = {Physics-informed machine learning in prognostics and health management},
	url = {https://www.sciencedirect.com/science/article/pii/S0307904X23003086},
	doi = {10.1016/j.apm.2023.07.011},
	abstract = {Prognostics and health management (PHM) plays a constructive role in the equipment’s entire life health service. It has long benefited from intensive research into physics modeling and machine learning methods. However, in practice, the existing solutions often encounter difficulties caused by sparse data \& incomplete system failure knowledge. Pure machine learning or physics-based methods can sometimes be infeasible in such situations. As a result, there has been a growing interest in developing physics-informed machine learning (PIML) models which allow incorporating different forms of physics knowledge at different positions of the machine learning pipeline. This combination provides significant assistance for detection, diagnostic, and prognostics. However, to the best of our knowledge, the bibliometrics analyses and the comprehensive review of the existing research concerning PIML in PHM remain vacant. Our review is therefore dedicated to filling these gaps. We synthesize the concept of PIML in PHM, and propose a taxonomy of PIML approaches from the perspective of “Expression forms of informed knowledge” and “Knowledge informed methods”. The findings and discussions presented in this paper enable us to clarify the current state of the art and the emerging opportunities of PIML approaches, especially for building PHM systems that can work under the “small data and scarce physics knowledge” paradigm.},
	urldate = {2024-02-19},
	journal = {Applied Mathematical Modelling},
	author = {Deng, Weikun and Nguyen, Khanh T. P. and Medjaher, Kamal and Gogu, Christian and Morio, Jérôme},
	month = dec,
	year = {2023},
	keywords = {Knowledge, Physics-constraint learning, Physics-embedded algorithm structure, Physics-informed input space, Physics-informed machine learning, Prognostics and health management},
	pages = {325--352},
	file = {1-s2.0-S0307904X23003086-main.pdf:/home/orincon/snap/zotero-snap/common/Zotero/storage/KWUJ2MBG/1-s2.0-S0307904X23003086-main.pdf:application/pdf;Exported Items.bib:/home/orincon/snap/zotero-snap/common/Zotero/storage/ALY9JBHI/Exported Items.bib:text/x-bibtex;ScienceDirect Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/YA2H5WZN/S0307904X23003086.html:text/html},
}

@article{vadyala_review_2022,
	title = {A review of physics-based machine learning in civil engineering},
	volume = {13},
	issn = {2590-1230},
	url = {https://www.sciencedirect.com/science/article/pii/S2590123021001171},
	doi = {10.1016/j.rineng.2021.100316},
	abstract = {The recent development of machine learning (ML) and Deep Learning (DL) increases the opportunities in all the sectors. ML is a significant tool that can be applied across many disciplines, but its direct application to civil engineering problems can be challenging. ML for civil engineering applications that are simulated in the lab often fail in real-world tests. This is usually attributed to a data mismatch between the data used to train and test the ML model and the data it encounters in the real world, a phenomenon known as data shift. However, a physics-based ML model integrates data, partial differential equations (PDEs), and mathematical models to solve data shift problems. Physics-based ML models are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear equations. Physics-based ML, which takes center stage across many science disciplines, plays an important role in fluid dynamics, quantum mechanics, computational resources, and data storage. This paper reviews the history of physics-based ML and its application in civil engineering.},
	urldate = {2024-02-19},
	journal = {Results in Engineering},
	author = {Vadyala, Shashank Reddy and Betgeri, Sai Nethra and Matthews, John C. and Matthews, Elizabeth},
	month = mar,
	year = {2022},
	keywords = {Civil engineering, Deep neural network, Machine learning, Physics-based machine learning},
	pages = {100316},
	file = {ScienceDirect Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/92SPAILV/S2590123021001171.html:text/html;Submitted Version:/home/orincon/snap/zotero-snap/common/Zotero/storage/G8U9TL5E/Vadyala et al. - 2022 - A review of physics-based machine learning in civi.pdf:application/pdf},
}

@article{blechschmidt_three_2021,
	title = {Three ways to solve partial differential equations with neural networks — {A} review},
	volume = {44},
	copyright = {© 2021 The Authors. GAMM - Mitteilungen published by Wiley-VCH GmbH.},
	issn = {1522-2608},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/gamm.202100006},
	doi = {10.1002/gamm.202100006},
	abstract = {Neural networks are increasingly used to construct numerical solution methods for partial differential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman–Kac formula and methods based on the solution of backward stochastic differential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.},
	language = {en},
	number = {2},
	urldate = {2024-02-22},
	journal = {GAMM-Mitteilungen},
	author = {Blechschmidt, Jan and Ernst, Oliver G.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/gamm.202100006},
	keywords = {backward differential equation, curse of dimensionality, Feynman–Kac, Hamilton–Jacobi–Bellman equations, neural networks, partial differential equation, PINN, stochastic process},
	pages = {e202100006},
	file = {Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/9BL4ZG4Y/Blechschmidt and Ernst - 2021 - Three ways to solve partial differential equations.pdf:application/pdf},
}

@misc{li_neural_2020,
	title = {Neural {Operator}: {Graph} {Kernel} {Network} for {Partial} {Differential} {Equations}},
	shorttitle = {Neural {Operator}},
	url = {http://arxiv.org/abs/2003.03485},
	doi = {10.48550/arXiv.2003.03485},
	abstract = {The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = mar,
	year = {2020},
	note = {arXiv:2003.03485 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/62G8G5RY/Li et al. - 2020 - Neural Operator Graph Kernel Network for Partial .pdf:application/pdf;arXiv.org Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/KFV6JKE2/2003.html:text/html},
}


@article{lin_physics-guided_2023,
	title = {Physics-{Guided} {Data}-{Driven} {Seismic} {Inversion}: {Recent} progress and future opportunities in full-waveform inversion},
	volume = {40},
	issn = {1558-0792},
	shorttitle = {Physics-{Guided} {Data}-{Driven} {Seismic} {Inversion}},
	url = {https://ieeexplore.ieee.org/document/10004771},
	doi = {10.1109/MSP.2022.3217658},
	abstract = {The goal of seismic inversion is to obtain subsurface properties from surface measurements. Seismic images have proven valuable, even crucial, for a variety of applications, including subsurface energy exploration, earthquake early warning, carbon capture and sequestration, estimating pathways of subsurface contaminant transport, etc. These subsurface properties (such as wave speed, density, and elastic velocities) influence the transmission of seismic waves through the subsurface media, and well-understood physics models (so-called “forward models”) can be used to predict what surface measurements would be made for any given subsurface configuration.},
	number = {1},
	urldate = {2024-02-19},
	journal = {IEEE Signal Processing Magazine},
	author = {Lin, Youzuo and Theiler, James and Wohlberg, Brendt},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Earthquakes, Measurement uncertainty, Predictive models, Seismic measurements, Surface contamination, Surface waves, Uncertainty},
	pages = {115--133},
	file = {IEEE Xplore Abstract Record:/home/orincon/snap/zotero-snap/common/Zotero/storage/Q6UVSUCJ/10004771.html:text/html;IEEE Xplore Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/UT5WHMF4/Lin et al. - 2023 - Physics-Guided Data-Driven Seismic Inversion Rece.pdf:application/pdf},
}

@article{yu_deep_2021,
	title = {Deep {Learning} for {Geophysics}: {Current} and {Future} {Trends}},
	volume = {59},
	copyright = {© 2021. The Authors.},
	issn = {1944-9208},
	shorttitle = {Deep {Learning} for {Geophysics}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021RG000742},
	doi = {10.1029/2021RG000742},
	abstract = {Recently deep learning (DL), as a new data-driven technique compared to conventional approaches, has attracted increasing attention in geophysical community, resulting in many opportunities and challenges. DL was proven to have the potential to predict complex system states accurately and relieve the “curse of dimensionality” in large temporal and spatial geophysical applications. We address the basic concepts, state-of-the-art literature, and future trends by reviewing DL approaches in various geosciences scenarios. Exploration geophysics, earthquakes, and remote sensing are the main focuses. More applications, including Earth structure, water resources, atmospheric science, and space science, are also reviewed. Additionally, the difficulties of applying DL in the geophysical community are discussed. The trends of DL in geophysics in recent years are analyzed. Several promising directions are provided for future research involving DL in geophysics, such as unsupervised learning, transfer learning, multimodal DL, federated learning, uncertainty estimation, and active learning. A coding tutorial and a summary of tips for rapidly exploring DL are presented for beginners and interested readers of geophysics.},
	language = {en},
	number = {3},
	urldate = {2024-02-21},
	journal = {Reviews of Geophysics},
	author = {Yu, Siwei and Ma, Jianwei},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021RG000742},
	keywords = {artificial intelligence, data-driven geophysics, deep learning, dictionary learning, exploration geophysics, machine learning},
	pages = {e2021RG000742},
	file = {Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/I3DHTHK2/Yu y Ma - 2021 - Deep Learning for Geophysics Current and Future T.pdf:application/pdf;Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/C4XGMN5P/2021RG000742.html:text/html},
}


@misc{kharazmi_variational_2019,
	title = {Variational {Physics}-{Informed} {Neural} {Networks} {For} {Solving} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1912.00873},
	doi = {10.48550/arXiv.1912.00873},
	abstract = {Physics-informed neural networks (PINNs) [31] use automatic differentiation to solve partial differential equations (PDEs) by penalizing the PDE in the loss function at a random set of points in the domain of interest. Here, we develop a Petrov-Galerkin version of PINNs based on the nonlinear approximation of deep neural networks (DNNs) by selecting the \{{\textbackslash}em trial space\} to be the space of neural networks and the \{{\textbackslash}em test space\} to be the space of Legendre polynomials. We formulate the {\textbackslash}textit\{variational residual\} of the PDE using the DNN approximation by incorporating the variational form of the problem into the loss function of the network and construct a {\textbackslash}textit\{variational physics-informed neural network\} (VPINN). By integrating by parts the integrand in the variational form, we lower the order of the differential operators represented by the neural networks, hence effectively reducing the training cost in VPINNs while increasing their accuracy compared to PINNs that essentially employ delta test functions. For shallow networks with one hidden layer, we analytically obtain explicit forms of the {\textbackslash}textit\{variational residual\}. We demonstrate the performance of the new formulation for several examples that show clear advantages of VPINNs over PINNs in terms of both accuracy and speed.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Kharazmi, E. and Zhang, Z. and Karniadakis, G. E.},
	month = nov,
	year = {2019},
	note = {arXiv:1912.00873 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Numerical Analysis, Physics - Computational Physics, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/FLU83LXQ/Kharazmi et al. - 2019 - Variational Physics-Informed Neural Networks For S.pdf:application/pdf;arXiv.org Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/LPYQ8F4E/1912.html:text/html},
}


@article{wu_helmholtz-equation_2023,
	title = {Helmholtz-equation solution in nonsmooth media by a physics-informed neural network incorporating quadratic terms and a perfectly matching layer condition},
	volume = {88},
	issn = {0016-8033},
	url = {https://doi.org/10.1190/geo2022-0479.1},
	doi = {10.1190/geo2022-0479.1},
	abstract = {Frequency-domain simulation of seismic waves plays an important role in seismic inversion, but it remains challenging in large models. The recently proposed physics-informed neural network (PINN), as an effective deep-learning method, has achieved successful applications in solving a wide range of partial differential equations (PDEs), and there is still room for improvement on this front. For example, PINN can lead to inaccurate solutions when PDE coefficients are nonsmooth and describe structurally complex media. Thus, we solve the acoustic and visco-acoustic scattered-field (Lippmann-Schwinger) wave equation in the frequency domain with PINN instead of the wave equation to remove the source singularity. We first illustrate that nonsmooth velocity models lead to inaccurate wavefields when no boundary conditions are implemented in the loss function. Then, we add the perfectly matched layer (PML) conditions in the loss function to better couple the real and imaginary parts of the wavefield. Moreover, we design new neurons by replacing the classical affine function with a quadratic function in the argument of the activation function to better capture the nonsmooth features of the wavefields. We find that the PML condition and the quadratic functions improve the results including handling attenuation and discuss the reason for this improvement. We also illustrate that a network trained to predict a wavefield for a specific medium can be used as an initial model of the neural network for predicting other wavefields corresponding to PDE-coefficient alterations and improving the convergence speed accordingly. This pretraining strategy should find applications in iterative full-waveform inversion and time-lag target-oriented imaging when the model perturbation between two consecutive iterations or two consecutive experiments is small.},
	number = {4},
	urldate = {2024-02-28},
	journal = {Geophysics},
	author = {Wu, Yanqi and Aghamiry, Hossein S. and Operto, Stephane and Ma, Jianwei},
	month = jun,
	year = {2023},
	pages = {T185--T202},
	file = {Snapshot:/home/orincon/snap/zotero-snap/common/Zotero/storage/FZ2NRDBI/Helmholtz-equation-solution-in-nonsmooth-media-by.html:text/html},
}


@inproceedings{alkhadhr_modeling_2021,
	title = {Modeling of the {Forward} {Wave} {Propagation} {Using} {Physics}-{Informed} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/abstract/document/9593574},
	doi = {10.1109/IUS52206.2021.9593574},
	abstract = {Partial Differential Equations (PDEs) are used in modeling problems in nature and are commonly solved using classical methods like Finite Element Method (FEM), Finite Volume Method (FVM), or Finite Difference Method (FDM). However, solving high-dimensional PDEs has been notoriously difficult due to the Curse of Dimensionality (CoD). Among the pool of hyperbolic PDEs, the wave equation in particular is the base for modeling various clinical applications and designing many solutions in the medical fields of therapeutic and diagnostic ultrasound. This draws attention to the importance of accurate and efficient simulation. In recent years, deep neural networks have been proposed to predict numerical solutions of PDEs. Within that context, Physics-Informed Neural Networks (PINNs) have surfaced as a powerful tool for modeling PDEs. We simulate a linear wave equation with a single time-dependent sinusoidal source function e.g.: sin (π t) using PINNs to model one of the most fundamental modeling equations in medical ultrasound applications. Results achieved are validated by an FDM solution with the same problem setup. After training, the PINN prediction takes an average time 47\% of the FDM time performed by MATLAB for the same simulation metrics (IC, BC, and domain range) on the same machine. Being a mesh-free approach, PINNs overcome the CoD which is one of the main challenges in traditional modeling methods.},
	urldate = {2024-02-26},
	booktitle = {2021 {IEEE} {International} {Ultrasonics} {Symposium} ({IUS})},
	author = {Alkhadhr, Shaikhah and Liu, Xilun and Almekkawy, Mohamed},
	month = sep,
	year = {2021},
	note = {ISSN: 1948-5727},
	keywords = {Propagation, Neural networks, Wave Equation, Forward Problem, Frequency division multiplexing, Mathematical models, Numerical Modelling, Numerical models, Physics-Informed Neural Networks, Training, Ultrasonic imaging},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/home/orincon/snap/zotero-snap/common/Zotero/storage/8DVPJYJJ/9593574.html:text/html;IEEE Xplore Full Text PDF:/home/orincon/snap/zotero-snap/common/Zotero/storage/XXV3B86F/Alkhadhr et al. - 2021 - Modeling of the Forward Wave Propagation Using Phy.pdf:application/pdf},
}


@article{alkhadhr_wave_2023,
	title = {Wave {Equation} {Modeling} via {Physics}-{Informed} {Neural} {Networks}: {Models} of {Soft} and {Hard} {Constraints} for {Initial} and {Boundary} {Conditions}},
	volume = {23},
	issn = {1424-8220},
	shorttitle = {Wave {Equation} {Modeling} via {Physics}-{Informed} {Neural} {Networks}},
	doi = {10.3390/s23052792},
	abstract = {Therapeutic ultrasound waves are the main instruments used in many noninvasive clinical procedures. They are continuously transforming medical treatments through mechanical and thermal effects. To allow for effective and safe delivery of ultrasound waves, numerical modeling methods such as the Finite Difference Method (FDM) and the Finite Element Method (FEM) are used. However, modeling the acoustic wave equation can result in several computational complications. In this work, we study the accuracy of using Physics-Informed Neural Networks (PINNs) to solve the wave equation when applying different combinations of initial and boundary conditions (ICs and BCs) constraints. By exploiting the mesh-free nature of PINNs and their prediction speed, we specifically model the wave equation with a continuous time-dependent point source function. Four main models are designed and studied to monitor the effects of soft or hard constraints on the prediction accuracy and performance. The predicted solutions in all the models were compared to an FDM solution for prediction error estimation. The trials of this work reveal that the wave equation modeled by a PINN with soft IC and BC (soft–soft) constraints reflects the lowest prediction error among the four combinations of constraints. © 2023 by the authors.},
	language = {English},
	number = {5},
	journal = {Sensors},
	author = {Alkhadhr, S. and Almekkawy, M.},
	year = {2023},
	keywords = {physics-informed neural networks, numerical modeling, ultrasound therapeutics, wave equation},
	file = {Texto completo:/home/orincon/snap/zotero-snap/common/Zotero/storage/NPF85GKG/Alkhadhr y Almekkawy - 2023 - Wave Equation Modeling via Physics-Informed Neural.pdf:application/pdf},
}

@article{saloma_computational_1993,
	title = {Computational complexity and the observation of physical signals},
	volume = {74},
	issn = {0021-8979},
	url = {https://doi.org/10.1063/1.354232},
	doi = {10.1063/1.354232},
	abstract = {The effects of computational complexity on the characteristics of a physical signal that is reconstructed from its representation of sampled data are analyzed. It is found that a more complex algorithm does not only require longer time to implement, but also yields an erroneous reconstruction. The reconstruction suffers from contrast degradation, phase shifts, and attenuation of details relative to the true signal. These unwanted effects are caused by the existence of spurious frequencies in the computed spectrum due to rounding‐off errors. The amplitude distribution of the spurious frequencies across the spectral bandwidth strongly depends on the number of data points handled and on the complexity of the particular reconstruction algorithm employed. Since the floating point representation of numbers in a computer is always finite, an upper limit exists in the maximum number of additions or multiplications required to compute a quantity reliably without errors.},
	number = {9},
	urldate = {2024-07-09},
	journal = {Journal of Applied Physics},
	author = {Saloma, Caesar},
	month = nov,
	year = {1993},
	pages = {5314--5319},
	file = {Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\5SM3QLMS\\Computational-complexity-and-the-observation-of.html:text/html},
}

@article{barron_universal_1993,
	title = {Universal approximation bounds for superpositions of a sigmoidal function},
	volume = {39},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/256500},
	doi = {10.1109/18.256500},
	abstract = {Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings.{\textless}{\textgreater}},
	number = {3},
	urldate = {2024-07-12},
	journal = {IEEE Transactions on Information Theory},
	author = {Barron, A.R.},
	month = may,
	year = {1993},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Approximation error, Artificial neural networks, Feedforward neural networks, Feeds, Fourier transforms, Information theory, Linear approximation, Neural networks, Statistical distributions, Statistics},
	pages = {930--945},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\orincon\\Zotero\\storage\\LVSAQ2VI\\256500.html:text/html},
}


@article{dwivedi_physics_2020,
	title = {Physics {Informed} {Extreme} {Learning} {Machine} ({PIELM})–{A} rapid method for the numerical solution of partial differential equations},
	volume = {391},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231219318144},
	doi = {10.1016/j.neucom.2019.12.099},
	abstract = {There has been rapid progress recently on the application of deep networks to the solution of partial differential equations, collectively labeled as Physics Informed Neural Networks (PINNs). In this paper, We develop Physics Informed Extreme Learning Machine (PIELM), a rapid version of PINNs which can be applied to stationary and time-dependent linear partial differential equations. We demonstrate that PIELM matches or exceeds the accuracy of PINNs on a range of problems. We also discuss the limitations of neural network-based approaches, including our PIELM, in the solution of PDEs on large domains and suggest an extension, a distributed version of our algorithm -- DPIELM. We show that DPIELM produces excellent results comparable to conventional numerical techniques in the solution of time-dependent problems. Collectively, this work contributes towards making the use of neural networks in the solution of partial differential equations in complex domains as a competitive alternative to conventional discretization techniques.},
	urldate = {2024-07-04},
	journal = {Neurocomputing},
	author = {Dwivedi, Vikas and Srinivasan, Balaji},
	month = may,
	year = {2020},
	keywords = {Partial differential equations, Extreme learning machine, Advection-Diffusion equation, Physics informed neural networks},
	pages = {96--118},
	file = {ScienceDirect Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\92B6I5DD\\S0925231219318144.html:text/html;Versión enviada:C\:\\Users\\orincon\\Zotero\\storage\\2MJCCF8H\\Dwivedi y Srinivasan - 2020 - Physics Informed Extreme Learning Machine (PIELM)–.pdf:application/pdf},
}

@article{dwivedi_normal_2021,
	title = {A {Normal} {Equation}-{Based} {Extreme} {Learning} {Machine} for {Solving} {Linear} {Partial} {Differential} {Equations}},
	volume = {22},
	issn = {1530-9827},
	url = {https://doi.org/10.1115/1.4051530},
	doi = {10.1115/1.4051530},
	abstract = {This paper develops an extreme learning machine for solving linear partial differential equations (PDEs) by extending the normal equations approach for linear regression. The normal equations method is typically used when the amount of available data is small. In PDEs, the only available ground truths are the boundary and initial conditions (BC and IC). We use the physics-based cost function use in state-of-the-art deep neural network-based PDE solvers called physics-informed neural network (PINN) to compensate for the small data. However, unlike PINN, we derive the normal equations for PDEs and directly solve them to compute the network parameters. We demonstrate our method’s feasibility and efficiency by solving several problems like function approximation, solving ordinary differential equations (ODEs), and steady and unsteady PDEs on regular and complicated geometries. We also highlight our method’s limitation in capturing sharp gradients and propose its domain distributed version to overcome this issue. We show that this approach is much faster than traditional gradient descent-based approaches and offers an alternative to conventional numerical methods in solving PDEs in complicated geometries.},
	number = {014502},
	urldate = {2024-07-04},
	journal = {Journal of Computing and Information Science in Engineering},
	author = {Dwivedi, Vikas and Srinivasan, Balaji},
	month = jul,
	year = {2021},
	file = {Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\DJEHTBZD\\A-Normal-Equation-Based-Extreme-Learning-Machine.html:text/html},
}

@article{raissi_hidden_2018,
	title = {Hidden physics models: {Machine} learning of nonlinear partial differential equations},
	volume = {357},
	issn = {0021-9991},
	shorttitle = {Hidden physics models},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999117309014},
	doi = {10.1016/j.jcp.2017.11.039},
	abstract = {While there is currently a lot of enthusiasm about “big data”, useful data is usually “small” and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from small data. In particular, we introduce hidden physics models, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier–Stokes, Schrödinger, Kuramoto–Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.},
	urldate = {2024-07-12},
	journal = {Journal of Computational Physics},
	author = {Raissi, Maziar and Karniadakis, George Em},
	month = mar,
	year = {2018},
	keywords = {Bayesian modeling, Fractional equations, Probabilistic machine learning, Small data, System identification, Uncertainty quantification},
	pages = {125--141},
	file = {ScienceDirect Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\FFJU96N2\\S0021999117309014.html:text/html;Versión enviada:C\:\\Users\\orincon\\Zotero\\storage\\B86VWNDH\\Raissi y Karniadakis - 2018 - Hidden physics models Machine learning of nonline.pdf:application/pdf},
}

@article{raissi_numerical_2018,
	title = {Numerical {Gaussian} {Processes} for {Time}-{Dependent} and {Nonlinear} {Partial} {Differential} {Equations}},
	volume = {40},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/17M1120762},
	doi = {10.1137/17M1120762},
	abstract = {Implicit-explicit (IMEX) schemes have been widely used, especially in conjunction with spectral methods, for the time integration of spatially discretized partial differential equations (PDEs) of diffusion-convection type. Typically, an implicit scheme is used for the diffusion term and an explicit scheme is used for the convection term. Reaction-diffusion problems can also be  approximated in this manner. In this work we systematically analyze the performance of such schemes, propose improved new schemes, and pay particular attention to their relative performance in the context of fast multigrid algorithms and of aliasing reduction for spectral methods.For the prototype linear advection-diffusion equation, a stability analysis for first-, second-, third-, and fourth-order multistep IMEX schemes is performed. Stable schemes permitting large time steps for a wide variety of problems and yielding appropriate decay of high frequency error modes are identified. Numerical experiments demonstrate that weak decay of high frequency modes can lead to extra iterations on the finest grid when using multigrid computations with finite difference spatial discretization, and to aliasing when using spectral collocation for spatial discretization. When this behavior occurs, use of weakly damping schemes such as the popular combination of Crank–Nicolson with second-order Adams–Bashforth is discouraged and better alternatives are proposed.Our findings are demonstrated on several examples.},
	number = {1},
	urldate = {2024-07-12},
	journal = {SIAM Journal on Scientific Computing},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = jan,
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {A172--A198},
	file = {Versión enviada:C\:\\Users\\orincon\\Zotero\\storage\\7VVI5YHJ\\Raissi et al. - 2018 - Numerical Gaussian Processes for Time-Dependent an.pdf:application/pdf},
}

 
@article{galiounas_battery_2022,
	title = {Battery state-of-charge estimation using machine learning analysis of ultrasonic signatures},
	volume = {10},
	issn = {2666-5468},
	url = {https://www.sciencedirect.com/science/article/pii/S2666546822000374},
	doi = {10.1016/j.egyai.2022.100188},
	abstract = {The potential of acoustic signatures to be used for State-of-Charge (SoC) estimation is demonstrated using artificial neural network regression models. This approach represents a streamlined method of processing the entire acoustic waveform instead of performing manual, and often arbitrary, waveform peak selection. For applications where computational economy is prioritised, simple metrics of statistical significance are used to formally identify the most informative waveform features. These alone can be exploited for SoC inference. It is further shown that signal portions representing both early and late interfacial reflections can correlate highly with the SoC and be of predictive value, challenging the more common peak selection methods which focus on the latter. Although later echoes represent greater through-thickness coverage, and are intuitively more information-rich, their presence is not guaranteed. Holistic waveform treatment offers a more robust approach to correlating acoustic signatures to electrochemical states. It is further demonstrated that transformation into the frequency domain can reduce the dimensionality of the problem significantly, while also improving the estimation accuracy. Most importantly, it is shown that acoustic signatures can be used as sole model inputs to produce highly accurate SoC estimates, without any complementary voltage information. This makes the method suitable for applications where redundancy and diversification of SoC estimation approaches is needed. Data is obtained experimentally from a 210 mAh LiCoO2/graphite pouch cell. Mean estimation errors as low as 0.75\% are achieved on a SoC scale of 0–100\%.},
	urldate = {2024-07-15},
	journal = {Energy and AI},
	author = {Galiounas, Elias and Tranter, Tom G. and Owen, Rhodri E. and Robinson, James B. and Shearing, Paul R. and Brett, Dan J. L.},
	month = nov,
	year = {2022},
	keywords = {Acoustic battery inspection, Artificial neural networks, Battery diagnostics, Machine learning, Mechanical-electrochemical correlation, Ultrasonic battery monitoring},
	pages = {100188},
	file = {ScienceDirect Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\SL5A42UE\\S2666546822000374.html:text/html},
}


@article{mccann_convolutional_2017,
	title = {Convolutional {Neural} {Networks} for {Inverse} {Problems} in {Imaging}: {A} {Review}},
	volume = {34},
	issn = {1558-0792},
	shorttitle = {Convolutional {Neural} {Networks} for {Inverse} {Problems} in {Imaging}},
	url = {https://ieeexplore.ieee.org/document/8103129},
	doi = {10.1109/MSP.2017.2739299},
	abstract = {In this article, we review recent uses of convolutional neural networks (CNNs) to solve inverse problems in imaging. It has recently become feasible to train deep CNNs on large databases of images, and they have shown outstanding performance on object classification and segmentation tasks. Motivated by these successes, researchers have begun to apply CNNs to the resolution of inverse problems such as denoising, deconvolution, superresolution, and medical image reconstruction, and they have started to report improvements over state-of-the-art methods, including sparsity-based techniques such as compressed sensing.},
	number = {6},
	urldate = {2024-07-15},
	journal = {IEEE Signal Processing Magazine},
	author = {McCann, Michael T. and Jin, Kyong Hwan and Unser, Michael},
	month = nov,
	year = {2017},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Computed tomography, Image reconstruction, Image resolution, Image segmentation, Inverse problems, Linear programming, Neural networks, Noise reduction},
	pages = {85--95},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\orincon\\Zotero\\storage\\364Q7SYJ\\8103129.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\orincon\\Zotero\\storage\\USACLAP9\\McCann et al. - 2017 - Convolutional Neural Networks for Inverse Problems.pdf:application/pdf},
}


@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/089360809190009T},
	doi = {10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	number = {2},
	urldate = {2024-07-15},
	journal = {Neural Networks},
	author = {Hornik, Kurt},
	month = jan,
	year = {1991},
	keywords = {() approximation, Activation function, Input environment measure, Multilayer feedforward networks, Smooth approximation, Sobolev spaces, Uniform approximation, Universal approximation capabilities},
	pages = {251--257},
	file = {ScienceDirect Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\N6SMNISY\\089360809190009T.html:text/html},
}

@article{moseley_fast_2018,
	title = {Fast approximate simulation of seismic waves with deep learning},
	url = {http://arxiv.org/abs/1807.06873},
	abstract = {We simulate the response of acoustic seismic waves in horizontally layered media using a deep neural network. In contrast to traditional finite-difference modelling techniques our network is able to directly approximate the recorded seismic response at multiple receiver locations in a single inference step, without needing to iteratively model the seismic wavefield through time. This results in an order of magnitude reduction in simulation time from the order of 1 s for FD modelling to the order of 0.1 s using our approach. Such a speed improvement could lead to real-time seismic simulation applications and benefit seismic inversion algorithms based on forward modelling, such as full waveform inversion. Our proof of concept deep neural network is trained using 50,000 synthetic examples of seismic waves propagating through different 2D horizontally layered velocity models. We discuss how our approach could be extended to arbitrary velocity models. Our deep neural network design is inspired by the WaveNet architecture used for speech synthesis. We also investigate using deep neural networks for simulating the full seismic wavefield and for carrying out seismic inversion directly.},
	author = {Moseley, Benjamin and Markham, Andrew and Nissen-Meyer, Tarje},
	month = jul,
	year = {2018},
	file = {Moseley et al. - 2018 - Fast approximate simulation of seismic waves with .pdf:C\:\\Users\\orincon\\Zotero\\storage\\AJYPF294\\Moseley et al. - 2018 - Fast approximate simulation of seismic waves with .pdf:application/pdf},
}


@article{komatitsch_introduction_1999,
	title = {Introduction to the spectral element method for three-dimensional seismic wave propagation},
	volume = {139},
	issn = {0956-540X},
	url = {https://doi.org/10.1046/j.1365-246x.1999.00967.x},
	doi = {10.1046/j.1365-246x.1999.00967.x},
	abstract = {We present an introduction to the spectral element method, which provides an innovative numerical approach to the calculation of synthetic seismograms in 3-D earth models. The method combines the flexibility of a finite element method with the accuracy of a spectral method. One uses a weak formulation of the equations of motion, which are solved on a mesh of hexahedral elements that is adapted to the free surface and to the main internal discontinuities of the model. The wavefield on the elements is discretized using high-degree Lagrange interpolants, and integration over an element is accomplished based upon the Gauss-Lobatto-Legendre integration rule. This combination of discretization and integration results in a diagonal mass matrix, which greatly simplifies the algorithm. We illustrate the great potential of the method by comparing it to a discrete wavenumber/reflectivity method for layer-cake models. Both body and surface waves are accurately represented, and the method can handle point force as well as moment tensor sources. For a model with very steep surface topography we successfully benchmark the method against an approximate boundary technique. For a homogeneous medium with strong attenuation we obtain excellent agreement with the analytical solution for a point force.},
	number = {3},
	urldate = {2024-07-15},
	journal = {Geophysical Journal International},
	author = {Komatitsch, Dimitri and Tromp, Jeroen},
	month = dec,
	year = {1999},
	pages = {806--822},
	file = {Full Text PDF:C\:\\Users\\orincon\\Zotero\\storage\\TAGS6BTV\\Komatitsch y Tromp - 1999 - Introduction to the spectral element method for th.pdf:application/pdf;Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\L3JEBIKT\\587264.html:text/html},
}


@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{chen2020neurodiffeq,
  title={NeuroDiffEq: A Python package for solving differential equations with neural networks},
  author={Chen, Feiyu and Sondak, David and Protopapas, Pavlos and Mattheakis, Marios and Liu, Shuheng and Agarwal, Devansh and Di Giovanni, Marco},
  journal={Journal of Open Source Software},
  volume={5},
  number={46},
  pages={1931},
  year={2020}
}

@inproceedings{bafghi_pinns-torch_2023,
	title = {{PINNs}-{Torch}: {Enhancing} {Speed} and {Usability} of {Physics}-{Informed} {Neural} {Networks} with {PyTorch}},
	shorttitle = {{PINNs}-{Torch}},
	url = {https://openreview.net/forum?id=nl1ZzdHpab},
	abstract = {Physics-informed neural networks (PINNs) stand out for their ability in supervised learning tasks that align with physical laws, especially nonlinear partial differential equations (PDEs). In this paper, we introduce "PINNs-Torch", a Python package that accelerates PINNs implementation using the PyTorch framework and streamlines user interaction by abstracting PDE issues. While we utilize PyTorch's dynamic computational graph for its flexibility, we mitigate its computational overhead in PINNs by compiling it to static computational graphs. In our assessment across 8 diverse examples, covering continuous, discrete, forward, and inverse configurations, naive PyTorch is slower than TensorFlow; however, when integrated with CUDA Graph and JIT compilers, training speeds can increase by up to 9 times relative to TensorFlow implementations. Additionally, through a real-world example, we highlight situations where our package might not deliver speed improvements. For community collaboration and future developments, our package code is accessible at: https://github.com/rezaakb/pinns-torch.},
	language = {en},
	urldate = {2024-07-16},
	author = {Bafghi, Reza Akbarian and Raissi, Maziar},
	month = oct,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\orincon\\Zotero\\storage\\78AXSAUD\\Bafghi y Raissi - 2023 - PINNs-Torch Enhancing Speed and Usability of Phys.pdf:application/pdf},
}

@article{lu2021deepxde,
  author  = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  title   = {{DeepXDE}: A deep learning library for solving differential equations},
  journal = {SIAM Review},
  volume  = {63},
  number  = {1},
  pages   = {208-228},
  year    = {2021},
  doi     = {10.1137/19M1274067}
}


@misc{https://doi.org/10.48550/arxiv.2107.09443,
  doi = {10.48550/ARXIV.2107.09443},
  url = {https://arxiv.org/abs/2107.09443},
  author = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},
  keywords = {Mathematical Software (cs.MS), Symbolic Computation (cs.SC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with Error Approximations},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@software{dimitri_komatitsch_2023_10415228,
  author       = {Dimitri Komatitsch and
                  Jeroen Tromp and
                  Hom Nath Gharti and
                  Daniel Peter and
                  Eduardo Valero Cano and
                  Etienne Bachmann and
                  Alexis Bottero and
                  Quentin Brissaud and
                  Bryant Chow and
                  Paul Cristini and
                  Congyue Cui and
                  Rene Gassmoeller and
                  Michael Gineste and
                  Felix Halpaap and
                  Eric Heien and
                  Jesus Labarta and
                  Matthieu Lefebvre and
                  Nicolas Le Goff and
                  Pieyre Le Loher and
                  Qiancheng Liu and
                  Qinya Liu and
                  Youshan Liu and
                  Zhaolun Liu and
                  David Luet and
                  Roland Martin and
                  Rene Matzen and
                  Ryan Modrak and
                  Christina Morency and
                  Masaru Nagaso and
                  Eric Rosenkrantz and
                  Herurisa Rusmanugroho and
                  Elliott Sales de Andrade and
                  Carl Tape and
                  Jean-Pierre Vilotte and
                  Zhinan Xie and
                  Zhendong Zhang},
  title        = {SPECFEM/specfem2d: SPECFEM2D v8.1.0},
  month        = dec,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v8.1.0},
  doi          = {10.5281/zenodo.10415228},
  url          = {https://doi.org/10.5281/zenodo.10415228}
}

@software{komatitsch_2024_10823181,
  author       = {Komatitsch, Dimitri and
                  Tromp, Jeroen and
                  Garg, Rahul and
                  Gharti, Hom Nath and
                  Nagaso, Masaru and
                  Oral, Elif and
                  Peter, Daniel and
                  Afanasiev, Michael and
                  Almada, Rafael and
                  Ampuero, Jean-Paul and
                  Bachmann, Etienne and
                  Bai, Kangchen and
                  Basini, Piero and
                  Beller, Stephen and
                  Bishop, Jordan and
                  Bissey, Francois and
                  Blitz, Celine and
                  Bottero, Alexis and
                  Bozdag, Ebru and
                  Casarotti, Emanuele and
                  Charles, Joseph and
                  Chen, Min and
                  Cristini, Paul and
                  Durochat, Clement and
                  Galvez Barron, Percy and
                  Gassmoeller, Rene and
                  Goeddeke, Dominik and
                  Grinberg, Leopold and
                  Gupta, Aakash and
                  Heien, Eric and
                  Hjoerleifsdottir, Vala and
                  Karakostas, Foivos and
                  Kientz, Sue and
                  Labarta, Jesus and
                  Le Goff, Nicolas and
                  Le Loher, Pieyre and
                  Lefebvre, Matthieu and
                  Liu, Qinya and
                  Liu, Youshan and
                  Luet, David and
                  Luo, Yang and
                  Maggi, Alessia and
                  Magnoni, Federica and
                  Martin, Roland and
                  Matzen, Rene and
                  McBain, G. D. and
                  McRitchie, Dennis and
                  Meschede, Matthias and
                  Messmer, Peter and
                  Michea, David and
                  Miller, David and
                  Modrak, Ryan and
                  Monteiller, Vadim and
                  Morency, Christina and
                  Nadh Somala, Surendra and
                  Nissen-Meyer, Tarje and
                  Pouget, Kevin and
                  Rietmann, Max and
                  Sales de Andrade, Elliott and
                  Savage, Brian and
                  Schuberth, Bernhard and
                  Sieminski, Anne and
                  Smith, James and
                  Strand, Leif and
                  Tape, Carl and
                  Valero Cano, Eduardo and
                  Videau, Brice and
                  Vilotte, Jean-Pierre and
                  Weng, Huihui and
                  Xie, Zhinan and
                  Zhang, Chang-Hua and
                  Zhu, Hejun},
  title        = {SPECFEM/specfem3d: SPECFEM3D v4.1.1},
  month        = mar,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v4.1.1},
  doi          = {10.5281/zenodo.10823181},
  url          = {https://doi.org/10.5281/zenodo.10823181}
}

@article{komatitsch_unsplit_2007,
	title = {An unsplit convolutional perfectly matched layer improved at grazing incidence for the seismic wave equation},
	volume = {72},
	issn = {0016-8033},
	url = {https://library.seg.org/doi/10.1190/1.2757586},
	doi = {10.1190/1.2757586},
	abstract = {The perfectly matched layer (PML) absorbing boundary condition has proven to be very efficient from a numerical point of view for the elastic wave equation to absorb both body waves with nongrazing incidence and surface waves. However, at grazing incidence the classical discrete PML method suffers from large spurious reflections that make it less efficient for instance in the case of very thin mesh slices, in the case of sources located close to the edge of the mesh, and/or in the case of receivers located at very large offset. We demonstrate how to improve the PML at grazing incidence for the differential seismic wave equation based on an unsplit convolution technique. The improved PML has a cost that is similar in terms of memory storage to that of the classical PML. We illustrate the efficiency of this improved convolutional PML based on numerical benchmarks using a finite-difference method on a thin mesh slice for an isotropic material and show that results are significantly improved compared with the classical PML technique. We also show that, as the classical PML, the convolutional technique is intrinsically unstable in the case of some anisotropic materials.},
	number = {5},
	urldate = {2024-07-16},
	journal = {GEOPHYSICS},
	author = {Komatitsch, Dimitri and Martin, Roland},
	month = sep,
	year = {2007},
	note = {Publisher: Society of Exploration Geophysicists},
	pages = {SM155--SM167},
}


@misc{mcgreivy_weak_2024,
	title = {Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations},
	url = {http://arxiv.org/abs/2407.07218},
	doi = {10.48550/arXiv.2407.07218},
	abstract = {One of the most promising applications of machine learning (ML) in computational physics is to accelerate the solution of partial differential equations (PDEs). The key objective of ML-based PDE solvers is to output a sufficiently accurate solution faster than standard numerical methods, which are used as a baseline comparison. We first perform a systematic review of the ML-for-PDE solving literature. Of articles that use ML to solve a fluid-related PDE and claim to outperform a standard numerical method, we determine that 79\% (60/76) compare to a weak baseline. Second, we find evidence that reporting biases, especially outcome reporting bias and publication bias, are widespread. We conclude that ML-for-PDE solving research is overoptimistic: weak baselines lead to overly positive results, while reporting biases lead to underreporting of negative results. To a large extent, these issues appear to be caused by factors similar to those of past reproducibility crises: researcher degrees of freedom and a bias towards positive results. We call for bottom-up cultural changes to minimize biased reporting as well as top-down structural reforms intended to reduce perverse incentives for doing so.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {McGreivy, Nick and Hakim, Ammar},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07218 [physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Fluid Dynamics},
	file = {arXiv Fulltext PDF:C\:\\Users\\orincon\\Zotero\\storage\\AS63EUY2\\McGreivy y Hakim - 2024 - Weak baselines and reporting biases lead to overop.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\MSDZ6ZYX\\2407.html:text/html},
}


@incollection{chaljub_spectral-element_2007,
	series = {Advances in {Wave} {Propagation} in {Heterogenous} {Earth}},
	title = {Spectral-element analysis in seismology},
	volume = {48},
	url = {https://www.sciencedirect.com/science/article/pii/S0065268706480079},
	abstract = {We present a review of the application of the spectral-element method to regional and global seismology. This technique is a high-order variational method that allows one to compute accurate synthetic seismograms in three-dimensional heterogeneous Earth models with deformed geometry. We first recall the strong and weak forms of the seismic wave equation with a particular emphasis set on fluid regions. We then discuss in detail how the conditions that hold on the boundaries, including coupling boundaries, are honored. We briefly outline the spectral-element discretization procedure and present the time-marching algorithm that makes use of the diagonal structure of the mass matrix. We show examples that illustrate the capabilities of the method and its interest in the context of the computation of three-dimensional synthetic seismograms.},
	urldate = {2024-07-17},
	booktitle = {Advances in {Geophysics}},
	publisher = {Elsevier},
	author = {Chaljub, Emmanuel and Komatitsch, Dimitri and Vilotte, Jean-Pierre and Capdeville, Yann and Valette, Bernard and Festa, Gaetano},
	editor = {Wu, Ru-Shan and Maupin, Valerie and Dmowska, Renata},
	month = jan,
	year = {2007},
	doi = {10.1016/S0065-2687(06)48007-9},
	keywords = {DtN operator, Elastodynamics, Global seismology, Numerical modeling, Perfectly Matched Layers, Potential formulation, Regional seismology, Self-gravitation, Spectral-element method, Surface waves, Synthetic seismograms, Topography},
	pages = {365--419},
	file = {ScienceDirect Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\8JMJKGFF\\S0065268706480079.html:text/html},
}



@article{pao_functional_1995,
	series = {Control and {Robotics}, {Part} {II}},
	title = {The functional link net and learning optimal control},
	volume = {9},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/092523129500066F},
	doi = {10.1016/0925-2312(95)00066-F},
	abstract = {We present a strategy for learning optimal control. The approach uses functional-link neural network implementations which have several beneficial properties giving advantages over the more common generalized delta rule implementations. The learning task is decomposed into three parts: identification and monitoring, one-step-ahead control generation, and control path optimization. Each of these parts is accomplished with its own functional-link net and these are coordinated to provide the real-time learning of the optimal control path.},
	number = {2},
	urldate = {2024-07-17},
	journal = {Neurocomputing},
	author = {Pao, Yoh-Han and Phillips, Stephen M.},
	month = oct,
	year = {1995},
	keywords = {Functional-link net, Neural net control, Optimal control, Real-time learning},
	pages = {149--164},
	file = {ScienceDirect Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\6LNLCZ4I\\092523129500066F.html:text/html},
}

@article{jingbo_research_2023,
	title = {Research progress of physics-informed neural network in seismic wave modeling},
	volume = {38},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1004-2903},
	url = {http://www.progeophys.cn//en/article/doi/10.6038/pg2023GG0142},
	doi = {10.6038/pg2023GG0142},
	abstract = {{\textless}p{\textgreater}Partial differential equations, as important tools to describe objective physical laws, can be applied to solve the most scientific and engineering problems. Solving wave equations is an important part of numerical simulation of seismic wave field in Geophysics, which plays a necessary role in migration imaging and inversion. Based on the complexity of the structure of exploration targets, the mathematical descriptions of seismic wave equations are facing challenges. Classical numerical simulation methods have some pending improvements, and the demand for new orthogonal algorithms has become more vigorous. Physics-Informed Neural Network(PINN) is effective to get approximate solutions of partial differential equations by neural networks. PDEs, initial and boundary conditions will be embedded into loss functions through automatic differentiation to minimize data-driven errors while satisfying the physical constraint conditions of PDEs. Based on the analysis of PINN and the summary of its current research status, this paper summarizes and analyzes its existing study of solving wave equations, looks forward to its possible research directions in the future, and explores the application of PINN in 1D acoustic wave equations.{\textless}/p{\textgreater}},
	language = {zh},
	number = {1},
	urldate = {2024-02-19},
	journal = {Progress in Geophysics},
	author = {JingBo, Z. O. U. and Cai, L. I. U. and PengFei, Zhao},
	month = feb,
	year = {2023},
	note = {Publisher: Progress in Geophysics},
	pages = {430--448},
	file = {JingBo et al. - 2023 - Research progress of physics-informed neural netwo.pdf:C\:\\Users\\orincon\\Zotero\\storage\\8YPWZ8EU\\JingBo et al. - 2023 - Research progress of physics-informed neural netwo.pdf:application/pdf},
}

@book{sander_dune_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computational} {Science} and {Engineering}},
	title = {{DUNE} — {The} {Distributed} and {Unified} {Numerics} {Environment}},
	volume = {140},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-030-59701-6 978-3-030-59702-3},
	url = {http://link.springer.com/10.1007/978-3-030-59702-3},
	language = {en},
	urldate = {2024-07-31},
	publisher = {Springer International Publishing},
	author = {Sander, Oliver},
	year = {2020},
	doi = {10.1007/978-3-030-59702-3},
	file = {Sander - 2020 - DUNE — The Distributed and Unified Numerics Enviro.pdf:C\:\\Users\\orincon\\Zotero\\storage\\AGGG8IZE\\Sander - 2020 - DUNE — The Distributed and Unified Numerics Enviro.pdf:application/pdf},
}


@misc{grossmann_can_2023,
	title = {Can {Physics}-{Informed} {Neural} {Networks} beat the {Finite} {Element} {Method}?},
	url = {http://arxiv.org/abs/2302.04107},
	doi = {10.48550/arXiv.2302.04107},
	abstract = {Partial differential equations play a fundamental role in the mathematical modelling of many processes and systems in physical, biological and other sciences. To simulate such processes and systems, the solutions of PDEs often need to be approximated numerically. The finite element method, for instance, is a usual standard methodology to do so. The recent success of deep neural networks at various approximation tasks has motivated their use in the numerical solution of PDEs. These so-called physics-informed neural networks and their variants have shown to be able to successfully approximate a large range of partial differential equations. So far, physics-informed neural networks and the finite element method have mainly been studied in isolation of each other. In this work, we compare the methodologies in a systematic computational study. Indeed, we employ both methods to numerically solve various linear and nonlinear partial differential equations: Poisson in 1D, 2D, and 3D, Allen-Cahn in 1D, semilinear Schr{\textbackslash}"odinger in 1D and 2D. We then compare computational costs and approximation accuracies. In terms of solution time and accuracy, physics-informed neural networks have not been able to outperform the finite element method in our study. In some experiments, they were faster at evaluating the solved PDE.},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Grossmann, Tamara G. and Komorowska, Urszula Julia and Latz, Jonas and Schönlieb, Carola-Bibiane},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04107 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:C\:\\Users\\orincon\\Zotero\\storage\\QU9ZPX3H\\Grossmann et al. - 2023 - Can Physics-Informed Neural Networks beat the Fini.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\2MHLZ4HE\\2302.html:text/html},
}

@misc{mcgreivy_weak_2024,
	title = {Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations},
	url = {http://arxiv.org/abs/2407.07218},
	doi = {10.48550/arXiv.2407.07218},
	abstract = {One of the most promising applications of machine learning (ML) in computational physics is to accelerate the solution of partial differential equations (PDEs). The key objective of ML-based PDE solvers is to output a sufficiently accurate solution faster than standard numerical methods, which are used as a baseline comparison. We first perform a systematic review of the ML-for-PDE solving literature. Of articles that use ML to solve a fluid-related PDE and claim to outperform a standard numerical method, we determine that 79\% (60/76) compare to a weak baseline. Second, we find evidence that reporting biases, especially outcome reporting bias and publication bias, are widespread. We conclude that ML-for-PDE solving research is overoptimistic: weak baselines lead to overly positive results, while reporting biases lead to underreporting of negative results. To a large extent, these issues appear to be caused by factors similar to those of past reproducibility crises: researcher degrees of freedom and a bias towards positive results. We call for bottom-up cultural changes to minimize biased reporting as well as top-down structural reforms intended to reduce perverse incentives for doing so.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {McGreivy, Nick and Hakim, Ammar},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07218 [physics]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Fluid Dynamics},
	file = {arXiv Fulltext PDF:C\:\\Users\\orincon\\Zotero\\storage\\NHKY3Q9K\\McGreivy y Hakim - 2024 - Weak baselines and reporting biases lead to overop.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\D8QYEN5Y\\2407.html:text/html},
}

@article{mehrkanoon_approximate_2012,
	title = {Approximate {Solutions} to {Ordinary} {Differential} {Equations} {Using} {Least} {Squares} {Support} {Vector} {Machines}},
	volume = {23},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/abstract/document/6224185},
	doi = {10.1109/TNNLS.2012.2202126},
	abstract = {In this paper, a new approach based on least squares support vector machines (LS-SVMs) is proposed for solving linear and nonlinear ordinary differential equations (ODEs). The approximate solution is presented in closed form by means of LS-SVMs, whose parameters are adjusted to minimize an appropriate error function. For the linear and nonlinear cases, these parameters are obtained by solving a system of linear and nonlinear equations, respectively. The method is well suited to solving mildly stiff, nonstiff, and singular ODEs with initial and boundary conditions. Numerical results demonstrate the efficiency of the proposed method over existing methods.},
	number = {9},
	urldate = {2024-08-01},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Mehrkanoon, Siamak and Falck, Tillmann and Suykens, Johan A. K.},
	month = sep,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Closed-form approximate solution, collocation method, Differential equations, Kernel, Least squares approximation, least squares support vector machines (LS-SVMs), Manganese, Mathematical model, Optimization, ordinary differential equations (ODEs)},
	pages = {1356--1367},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\orincon\\Zotero\\storage\\PP5SU5HD\\6224185.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\orincon\\Zotero\\storage\\MBDRW3IM\\Mehrkanoon et al. - 2012 - Approximate Solutions to Ordinary Differential Equ.pdf:application/pdf},
}


@book{durran_numerical_2013,
	title = {Numerical {Methods} for {Wave} {Equations} in {Geophysical} {Fluid} {Dynamics}},
	isbn = {978-1-4757-3081-4},
	abstract = {Mathematics is playing an ever more important role in the physical and biological sciences, provoking a blurring of boundaries between scientific disciplines and a resurgence of interest in the modem as weIlas the classical techniques of applied mathematics. This renewal of interest, both in research and teaching, has led to the establishment of the series: Texts in AppliedMathematics (TAM). The development of new courses is a natural consequence of a high level of excitement on the research frontier as newer techniques, such as numerical and symbolic computer systems, dynamical systems, and chaos, mix with and rein force the traditional methods of applied mathematics. Thus, the purpose of this textbook series is to meet the current and future needs of these advances and en courage the teaching of new courses. TAM will publish textbooks suitable for use in advanced undergraduate and beginning graduate courses, and will complement the AppliedMathematical Sei ences (AMS) series, which will focus on advanced textbooks and research level monographs. Preface This book is designed to serve as a textbook for graduate students or advanced undergraduates studying numerical methods for the solution of partial differen tial equations goveming wave-like flows. Although the majority of the schemes presented in this text were introduced ineither the applied-rnathematics or atmos pheric-science literature, the focus is not on the nuts-and-bolts details of various atmospheric models but on fundamental numerical methods that have applications in a wide range of scientific and engineering disciplines.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Durran, Dale R.},
	month = mar,
	year = {2013},
	note = {Google-Books-ID: JioBCAAAQBAJ},
	keywords = {Mathematics / Algebra / General, Mathematics / Number Systems, Mathematics / Numerical Analysis, Science / Physics / Geophysics},
}


@article{mehrkanoon_learning_2015,
	title = {Learning solutions to partial differential equations using {LS}-{SVM}},
	volume = {159},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231215001629},
	doi = {10.1016/j.neucom.2015.02.013},
	abstract = {This paper proposes an approach based on Least Squares Support Vector Machines (LS-SVMs) for solving second order partial differential equations (PDEs) with variable coefficients. Contrary to most existing techniques, the proposed method provides a closed form approximate solution. The optimal representation of the solution is obtained in the primal–dual setting. The model is built by incorporating the initial/boundary conditions as constraints of an optimization problem. The developed method is well suited for problems involving singular, variable and constant coefficients as well as problems with irregular geometrical domains. Numerical results for linear and nonlinear PDEs demonstrate the efficiency of the proposed method over existing methods.},
	urldate = {2024-08-01},
	journal = {Neurocomputing},
	author = {Mehrkanoon, Siamak and Suykens, Johan A. K.},
	month = jul,
	year = {2015},
	keywords = {Closed form approximate solution, Collocation method, Least squares support vector machines, Partial differential equations},
	pages = {105--116},
}

@article{haghighat_physics-informed_2021,
	title = {A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics},
	volume = {379},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521000773},
	doi = {10.1016/j.cma.2021.113741},
	abstract = {We present the application of a class of deep learning, known as Physics Informed Neural Networks (PINN), to inversion and surrogate modeling in solid mechanics. We explain how to incorporate the momentum balance and constitutive relations into PINN, and explore in detail the application to linear elasticity, and illustrate its extension to nonlinear problems through an example that showcases von Mises elastoplasticity. While common PINN algorithms are based on training one deep neural network (DNN), we propose a multi-network model that results in more accurate representation of the field variables. To validate the model, we test the framework on synthetic data generated from analytical and numerical reference solutions. We study convergence of the PINN model, and show that Isogeometric Analysis (IGA) results in superior accuracy and convergence characteristics compared with classic low-order Finite Element Method (FEM). We also show the applicability of the framework for transfer learning, and find vastly accelerated convergence during network re-training. Finally, we find that honoring the physics leads to improved robustness: when trained only on a few parameters, we find that the PINN model can accurately predict the solution for a wide range of parameters new to the network—thus pointing to an important application of this framework to sensitivity analysis and surrogate modeling.},
	urldate = {2024-08-03},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Haghighat, Ehsan and Raissi, Maziar and Moure, Adrian and Gomez, Hector and Juanes, Ruben},
	month = jun,
	year = {2021},
	keywords = {Artificial neural network, Elastoplasticity, Inversion, Linear elasticity, Physics-informed deep learning, Transfer learning},
	pages = {113741},
	file = {ScienceDirect Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\8EGDYB8B\\S0045782521000773.html:text/html},
}

@article{raissi_hidden_2020,
	title = {Hidden fluid mechanics: {Learning} velocity and pressure fields from flow visualizations},
	volume = {367},
	shorttitle = {Hidden fluid mechanics},
	url = {https://www.science.org/doi/full/10.1126/science.aaw4741},
	doi = {10.1126/science.aaw4741},
	abstract = {For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.},
	number = {6481},
	urldate = {2024-08-03},
	journal = {Science},
	author = {Raissi, Maziar and Yazdani, Alireza and Karniadakis, George Em},
	month = feb,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1026--1030},
	file = {Full Text PDF:C\:\\Users\\orincon\\Zotero\\storage\\V875FFMY\\Raissi et al. - 2020 - Hidden fluid mechanics Learning velocity and pres.pdf:application/pdf},
}

@article{roth_neural_1994,
	title = {Neural networks and inversion of seismic data},
	volume = {99},
	copyright = {Copyright 1994 by the American Geophysical Union.},
	issn = {2156-2202},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/93JB01563},
	doi = {10.1029/93JB01563},
	abstract = {Neural networks can be viewed as applications that map one space, the input space, into some output space. In order to simulate the desired mapping the network has to go through a learning process consisting of an iterative change of the internal parameters, through the presentation of many input patterns and their corresponding output patterns. The training process is accomplished if the error between the computed output and the desired output pattern is minimal for all examples in the training set. The network will then simulate the desired mapping on the restricted domain of the training examples. We describe an experiment where a neural network is designed to accept a synthetic common shot gather (i.e., a set of seismograms obtained from a single source), as its input pattern and to compute the corresponding one-dimensional large-scale velocity model as its output. The subsurface models are built up of eight layers with constant layer thickness over a homogeneous half-space, 450 examples are used to train the network. After the training process the network never computes a subsurface model which perfectly fits the desired one, but the approximation of the network is sufficient to take this model as starting model for further seismic imaging algorithms. The trained network computes satisfactory velocity profiles for 80\% of the new seismic gathers not included in the training set. Although the network gives results that are stable when the input is contaminated with white noise, the network is not robust against strong, i.e., correlated, noise. This application proves that neural networks are able to solve nontrivial inverse problems.},
	language = {en},
	number = {B4},
	urldate = {2024-08-09},
	journal = {Journal of Geophysical Research: Solid Earth},
	author = {Röth, Gunter and Tarantola, Albert},
	year = {1994},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/93JB01563},
	pages = {6753--6768},
	file = {Röth y Tarantola - 1994 - Neural networks and inversion of seismic data.pdf:C\:\\Users\\orincon\\Zotero\\storage\\EF4DYAA5\\Röth y Tarantola - 1994 - Neural networks and inversion of seismic data.pdf:application/pdf;Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\HP2Y7A5Z\\93JB01563.html:text/html},
}


@misc{hao_physics-informed_2023,
	title = {Physics-{Informed} {Machine} {Learning}: {A} {Survey} on {Problems}, {Methods} and {Applications}},
	shorttitle = {Physics-{Informed} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2211.08064},
	abstract = {Recent advances of data-driven machine learning have revolutionized ﬁelds like computer vision, reinforcement learning, and many scientiﬁc and engineering domains. In many real-world and scientiﬁc problems, systems that generate data are governed by physical laws. Recent work shows that it provides potential beneﬁts for machine learning models by incorporating the physical prior and collected data, which makes the intersection of machine learning and physics become a prevailing paradigm. By integrating the data and mathematical physics models seamlessly, it can guide the machine learning model towards solutions that are physically plausible, improving accuracy and efﬁciency even in uncertain and high-dimensional contexts. In this survey, we present this learning paradigm called Physics-Informed Machine Learning (PIML) which is to build a model that leverages empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism. We systematically review the recent development of physics-informed machine learning from three perspectives of machine learning tasks, representation of physical prior, and methods for incorporating physical prior. We also propose several important open research problems based on the current trends in the ﬁeld. We argue that encoding different forms of physical prior into model architectures, optimizers, inference algorithms, and signiﬁcant domain-speciﬁc applications like inverse engineering design and robotic control is far from being fully explored in the ﬁeld of physics-informed machine learning. We believe that the interdisciplinary research of physics-informed machine learning will signiﬁcantly propel research progress, foster the creation of more effective machine learning models, and also offer invaluable assistance in addressing long-standing problems in related disciplines.},
	language = {en},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Hao, Zhongkai and Liu, Songming and Zhang, Yichi and Ying, Chengyang and Feng, Yao and Su, Hang and Zhu, Jun},
	month = mar,
	year = {2023},
	note = {arXiv:2211.08064 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {Hao et al. - 2023 - Physics-Informed Machine Learning A Survey on Pro.pdf:C\:\\Users\\orincon\\Zotero\\storage\\M6LPHPNL\\Hao et al. - 2023 - Physics-Informed Machine Learning A Survey on Pro.pdf:application/pdf},
}


@article{song_high-frequency_2022,
	title = {High-frequency wavefield extrapolation using the {Fourier} neural operator},
	volume = {19},
	issn = {1742-2132},
	url = {https://doi.org/10.1093/jge/gxac016},
	doi = {10.1093/jge/gxac016},
	abstract = {In seismic wave simulation, solving the wave equation in the frequency domain requires calculating the inverse of the impedance matrix. The total cost strictly depends on the number of frequency components that are considered, if using a finite-difference method. For the applications such as seismic imaging and inversion, high-frequency information is always required and thus the wave simulation is always a challenging task as it demands tremendous computational cost for obtaining dispersion-free high-frequency wavefields for large subsurface models. This paper demonstrates that a data-driven machine learning method, called the Fourier neural operator (FNO), is capable of predicting high-frequency wavefields, based on a limited number of low-frequency components. As the FNO method is for the first time applied to seismic wavefield extrapolation, the experiment reveals three attractive features with FNO: high efficiency, high accuracy and, importantly, the predicted high-frequency wavefields are dispersion free.},
	number = {2},
	urldate = {2024-08-12},
	journal = {Journal of Geophysics and Engineering},
	author = {Song, Chao and Wang, Yanghua},
	month = apr,
	year = {2022},
	pages = {269--282},
	file = {Full Text PDF:C\:\\Users\\orincon\\Zotero\\storage\\YFW36U5Z\\Song y Wang - 2022 - High-frequency wavefield extrapolation using the F.pdf:application/pdf;Snapshot:C\:\\Users\\orincon\\Zotero\\storage\\2KHRU2TW\\6576250.html:text/html},
}
@article{zhang_elastic_2024,
	title = {Elastic full-waveform inversion using tools of neural networks},
	volume = {99},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196871564&doi=10.1088%2f1402-4896%2fad55be&partnerID=40&md5=bedeada898893e4b0a548b24949d7f37},
	doi = {10.1088/1402-4896/ad55be},
	number = {7},
	journal = {Physica Scripta},
	author = {Zhang, W. and Chen, Z.},
	year = {2024},
	keywords = {comparative ML and SNM},
	annote = {Export Date: 24 July 2024; Cited By: 0},
	annote = {Summary


Comparative: Yes


MLM: RNN


PDE: 2-D elastic wave equation


Application: FWI


SNM: FEM



},
	file = {Zhang y Chen - 2024 - Elastic full-waveform inversion using tools of neu.pdf:C\:\\Users\\orincon\\Zotero\\storage\\K6W99TIX\\Zhang y Chen - 2024 - Elastic full-waveform inversion using tools of neu.pdf:application/pdf},
}